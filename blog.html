<!DOCTYPE html>
<html>
  <head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-70SVKC0TJP"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-70SVKC0TJP');
  </script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Ujjwal Upadhyay - blog</title>
  <meta name="description" content="Personal webpage">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <link rel="stylesheet" type="text/css" href="/css/icomoon.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="/blog">

  <link rel="alternate" type="application/rss+xml" title="Ujjwal Upadhyay" href="/feed.xml" />
  <link rel="icon" type="image/svg" href="/assets/img/infinite.svg">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="blog" />
<meta name="author" content="Ujjwal Upadhyay" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal webpage" />
<meta property="og:description" content="Personal webpage" />
<meta property="og:site_name" content="Ujjwal Upadhyay" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="blog" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Ujjwal Upadhyay"},"description":"Personal webpage","headline":"blog","url":"/blog"}</script>
<!-- End Jekyll SEO tag -->


  

  

  

  
</head>

  <body class="full-width">
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <ul class="topnav" style="float:left">
            <li><a style="padding-top:.5em; font-size:1.6rem; letter-spacing:0.2rem;" href="/">Ujjwal Upadhyay</a></li>
        </ul>
	<ul class="topnav">    
        
	
            
            
	        
                <li><a class="right" href="/">About</a></li>
	        
	    
            
  	
            
            
            
  	
            
            
	        
                <li><a class="right" href="/research">Research</a></li>
	        
	    
            
  	
            
            
	        
                <li><a class="right" href="/writings">Blog</a></li>
	        
	    
            
  	
            
            
	        
                <li><a class="right" href="/scratchpad">Random</a></li>
	        
	    
            
  	
            
  	
            
  	
            
  	
        <!-- <li><a class="right" href="https://drive.google.com/file/d/13Hxj3cRTK9nMl54z58sPOoOah6iEtw-C/view?usp=sharing">cv</a></li> -->
	</ul>
	</nav>
        <hr class="slender">
</header>

    <article>
    
      <p><br /></p>
<ul class="content-listing ">
          
        <li class="listing">
          
          <a href="/articles/24/intro-to-true-intelligence"><h3 class="contrast">True Intelligence - Part 1</h3></a>
          <br /><span class="smaller">December 1, 2024</span>  <br />
          <div><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 110}
});
</script>

<h1 id="abstract">Abstract</h1>

<p>True intelligence lies in the ability to adapt to the unknown. This poses one of the most enduring challenges in artificial intelligence: how to create systems capable of reasoning and solving problems beyond their initial training data. While AI systems excel in specific tasks with clear data boundaries, they often falter when presented with novel problems or contexts, such as rare medical conditions or unseen scenarios in robotics. The essence of intelligence, both human and artificial, involves compositional reasoning—the ability to deconstruct tasks into core components and recombine them in new contexts.</p>

<p>In this blog, we explore the principles of compositional understanding, the theoretical underpinnings of true intelligence, and potential approaches to achieving adaptability in artificial systems.</p>

<h1 id="the-challenge-of-adaptation">The Challenge of Adaptation</h1>

<p>The human mind excels at navigating unknown situations. For instance, a clinician encountering an unfamiliar disease can rely on foundational medical knowledge and observations to draw meaningful conclusions. In contrast, many AI models are brittle, trained to excel only within narrowly defined distributions.</p>

<p>The challenge can be mathematically framed as follows:</p>

<p>Given an input space \(X\), a task space \(T\), and a distribution \(P(X, T)\) over these spaces, an AI model \(M\) is designed to map inputs \(x \in X\) to outputs \(t \in T\). True intelligence requires \(M\) to generalize to distributions \(P'(X, T)\), where \(P'(X, T)\) may be significantly different from \(P(X, T)\). This requires learning core abstractions \(A\) such that:</p>

\[\text{Adaptation: } f: A \times P'(X, T) \to M'\]

<p>where \(M'\) is the adapted model for the new distribution.</p>

<h1 id="compositional-reasoning">Compositional Reasoning</h1>

<p>Humans achieve generalization by leveraging compositional reasoning. This involves identifying atomic concepts or sub-problems and combining them to solve complex tasks. In mathematical terms, compositional reasoning can be expressed as:</p>

\[M(x) = f\Big(\sum_{i=1}^{n} g_i(x)\Big)\]

<p>where \(g_i(x)\) represents atomic functions capturing fundamental logic or features, and \(f\) is the recombination function that enables solving a new problem.</p>

<p>For example:</p>
<ol>
  <li>Recognizing individual shapes (\(g_i(x)\)) in a visual scene.</li>
  <li>Recombining these shapes (\(f\)) to infer the presence of an object.</li>
</ol>

<p>This approach mirrors how humans decompose problems like understanding a sentence by identifying words, syntax, and semantics.</p>

<h1 id="bottlenecks-in-current-ai-systems">Bottlenecks in Current AI Systems</h1>

<p>Current AI systems primarily rely on pattern recognition, which struggles with:</p>
<ol>
  <li><strong>Out-of-distribution scenarios</strong>: Rare or novel cases are often misclassified.</li>
  <li><strong>Spurious correlations</strong>: Systems often mistake correlation for causation.</li>
  <li><strong>Lack of interpretability</strong>: Deep learning models often function as “black boxes.”</li>
</ol>

<p>To move beyond these limitations, we must design systems that exhibit reasoning rather than rote memorization.</p>

<h1 id="the-path-to-true-intelligence">The Path to True Intelligence</h1>

<p>True intelligence requires overcoming the combinatorial explosion of possibilities in real-world tasks. Consider program synthesis as an analogy: generating all possible programs that solve a task is infeasible. Instead, humans use intuition and abstraction to navigate vast solution spaces.</p>

<h2 id="abstraction-and-generalization">Abstraction and Generalization</h2>

<p>Abstractions are reusable building blocks that form the foundation of reasoning. They can be conceptualized as:</p>

\[A = \{ a_1, a_2, \dots, a_k \}\]

<p>where \(a_i\) represents a primitive abstraction derived from past experiences. A system capable of generalization must:</p>
<ol>
  <li>Learn \(A\) from data.</li>
  <li>Recombine \(A\) to form solutions for unseen problems.</li>
</ol>

<h2 id="framework-for-compositional-ai">Framework for Compositional AI</h2>

<p>We propose a dual-phase framework:</p>
<ol>
  <li><strong>Abstraction Generation</strong>: Discover core abstractions using techniques like clustering, unsupervised learning, or symbolic representation.</li>
  <li><strong>Synthesis and Reasoning</strong>: Use abstraction banks to compose solutions for novel tasks.</li>
</ol>

<p>The formal process is:</p>

\[S(x, T) = f(A, T)\]

<p>where \(S(x, T)\) synthesizes solutions \(x \to T\) using abstractions \(A\) and recombination logic \(f\).</p>

<h1 id="applications-in-high-stakes-domains">Applications in High-Stakes Domains</h1>

<p>Compositional AI is particularly relevant in high-stakes domains like healthcare. For example, in stroke detection:</p>
<ol>
  <li><strong>Abstraction</strong>: Identify features such as gaze deviation or ischemic regions.</li>
  <li><strong>Reasoning</strong>: Combine these features to diagnose rare stroke patterns.</li>
</ol>

<p>Similarly, in robotics, compositional reasoning allows systems to adapt to dynamic environments by recombining learned motor skills and sensory processing.</p>

<h1 id="toward-explainability">Toward Explainability</h1>

<p>An essential requirement for true intelligence is interpretability. A system’s reasoning process must be transparent and explainable, enabling users to trust its decisions. This can be achieved through structured outputs, such as:</p>

<ol>
  <li><strong>Semantic annotations</strong>: Annotating decisions with explanations tied to abstractions.</li>
  <li><strong>Visual grounding</strong>: Linking outputs to visual or sensory evidence.</li>
  <li><strong>Structured reasoning graphs</strong>: Representing inference paths as directed acyclic graphs (DAGs).</li>
</ol>

<h1 id="the-future-of-ai-research">The Future of AI Research</h1>

<p>True intelligence is the next frontier in AI. It requires bridging the gap between pattern recognition and reasoning through abstractions and composition. The future lies in:</p>
<ol>
  <li>Developing datasets and benchmarks that emphasize compositional complexity.</li>
  <li>Designing architectures that integrate deep learning with symbolic reasoning.</li>
  <li>Incorporating feedback loops for real-time adaptation and learning.</li>
</ol>

<p>True intelligence is not just about solving problems but understanding them in a way that allows adaptation to new, unknown challenges. By pursuing compositional AI, we take a significant step toward systems that mirror the adaptability and creativity of human cognition.</p>

<blockquote>
  <p><strong>Follow me on Twitter <a href="https://twitter.com/theujjwal9">@theujjwal9</a></strong></p>
</blockquote>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/23/intro-to-diffusion-model"><h3 class="contrast">Introduction to Diffusion Models</h3></a>
          <br /><span class="smaller">October 12, 2023</span>  <br />
          <div><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 110}
});
</script>

<p>Diffusion models have emerged as a powerful class of generative models, particularly in generating high-quality images. They operate by progressively adding noise to training data and then learning to reverse this process, effectively generating new data samples from random noise. This approach is deeply rooted in concepts from statistical physics and stochastic differential equations (SDEs).</p>

<h2 id="generative-modeling-and-the-role-of-the-score-function">Generative Modeling and the Role of the Score Function</h2>

<p>In generative modeling, our goal is to estimate a data distribution \(q_{\text{data}}(x)\) using a model \(p_{\theta}(x)\) parameterized by \(\theta\). A key concept in this context is the <em>score function</em>, defined as the gradient of the log-density of the data distribution:</p>

\[s(x) = \nabla_x \log q_{\text{data}}(x)\]

<p>This score function provides the direction in which the data density increases most rapidly and is instrumental in guiding the generation of new samples.</p>

<h2 id="langevin-dynamics-and-the-fokker-planck-equation">Langevin Dynamics and the Fokker-Planck Equation</h2>

<p>Langevin dynamics describe the evolution of a system under both deterministic forces and stochastic noise. In the context of diffusion models, they are used to sample from a distribution by iteratively updating samples with both gradient information (from the score function) and Gaussian noise:</p>

\[x_{t+1} = x_t + \frac{\epsilon}{2} s(x_t) + \sqrt{\epsilon} z_t\]

<p>where \(\epsilon\) is a step size and \(z_t \sim \mathcal{N}(0, I)\) is Gaussian noise. The corresponding Fokker-Planck equation describes the time evolution of the probability density function of the system’s state, ensuring that, under appropriate conditions, the samples converge to the target distribution.</p>

<h2 id="constructing-the-forward-and-reverse-processes">Constructing the Forward and Reverse Processes</h2>

<p>Diffusion models define a forward process that gradually adds noise to the data, transforming it into a simple prior distribution (e.g., Gaussian). This process can be modeled as a stochastic differential equation:</p>

\[dx = f(x, t)\, dt + g(t)\, dW_t\]

<p>where \(f(x, t)\) is a drift term, \(g(t)\) is a diffusion coefficient, and \(W_t\) represents a Wiener process. The reverse process, which aims to denoise and generate new samples, follows a similar SDE but with time running backward.</p>

<h2 id="estimating-the-score-function">Estimating the Score Function</h2>

<p>A critical challenge in diffusion models is accurately estimating the score function \(s(x)\). One effective method is <em>denoising score matching</em>, which involves training a neural network to predict the gradient of the log-density of noisy data. The objective function for this training is:</p>

\[\mathbb{E}_{q_{\sigma}(x)} \left[ \frac{1}{2} \left\| s_{\theta}(x) - \nabla_x \log q_{\sigma}(x) \right\|^2 \right]\]

<p>where \(q_{\sigma}(x)\) is the distribution of data corrupted with noise of level \(\sigma\). By minimizing this objective, the model learns to approximate the true score function, enabling effective sample generation.</p>

<h2 id="training-and-sampling-with-diffusion-models">Training and Sampling with Diffusion Models</h2>

<p>Training a diffusion model involves two main steps:</p>

<ol>
  <li>
    <p><strong>Defining a Noise Schedule</strong>: Set a schedule for the noise levels to be added during the forward process, typically increasing over time.</p>
  </li>
  <li>
    <p><strong>Learning to Denoise</strong>: Train a neural network to reverse the noising process by estimating the score function at various noise levels.</p>
  </li>
</ol>

<p>Once trained, sampling from the model is achieved by starting with random noise and iteratively applying the learned reverse process to generate data samples that resemble the training data distribution.</p>

<blockquote>
  <p><strong>Follow me on Twitter <a href="https://twitter.com/theujjwal9">@theujjwal9</a></strong></p>
</blockquote>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/22/intro-to-lean"><h3 class="contrast">Introduction to Lean</h3></a>
          <br /><span class="smaller">December 11, 2022</span>  <br />
          <div><p>Lean is an open source proof assistant developed by Microsoft Research.</p>

<p><a href="https://github.com/ujjwal-9/theorems">[Github repo for this blog series]</a></p>

<h1 id="introduction">Introduction</h1>

<p>Mathematics is characterised by the inferences allowed in the justification for the statements. The justification of one mathematician can be checked by
another by checking that each inference is between those allowed.</p>

<p>Mathematicians usually write proofs in natural languages using some special symbols to denote mathematical operations (\(\int\) - Integration) and objects (\(e\) - Euler’s number). Logicians have agreed upon <a href="https://en.wikipedia.org/wiki/List_of_rules_of_inference">rules of inference</a> which supports validity of proof.</p>

<p>Since these rules are mechanical, the process of checking is also mechanical as of now. But with advent of interactive theorem proving assistant it is possible to represent these proof in a manner that a machine can verify. One such proof assistant is lean (among other like Coq).</p>

<h1 id="background-on-lean">Background on Lean</h1>

<blockquote>
  <p>Lean = Functional Programming + Logic</p>
</blockquote>

<p>Lean encodes formal language in a version of <strong>dependent type theory</strong> (alternative to set theory) called <em>Calculus of Constructions</em>, with a countable hierarchy of non-cumulative universes and inductive types.</p>

<p>The only two things Lean can do is:</p>
<ol>
  <li>create terms</li>
  <li>check their types</li>
</ol>

<p>By iterating these two operations one can teach Lean to verify complex mathematical proofs.</p>

<p>Lets first look at simple type theory.</p>

<h2 id="simple-type-theory">Simple Type Theory</h2>

<p>Everything is a set, including numbers, functions, triangles, stochastic processes, and Riemannian manifolds. Using these sets we can construct rich mathematical intutions. But it will be helpful if we can manage and keep track of the various kinds of mathematical objects we are working with.</p>

<p><em>Type theory</em> states that every mathematical expression has a type. For example, \(x\) may denote natural numbers and \(f(x)\) may denote function on natural numbers maping them to lets say complex numbers. Such types conversions also make simple type theory even more powerful.</p>

<p>Lets see how we declare mathematical objects in lean and declare their types.</p>

<div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">constant</span> <span class="n">m</span> : <span class="n">nat</span><span class="cd">        -- m is a natural number</span>
<span class="k">constants</span> <span class="n">b1</span> <span class="n">b2</span> : <span class="n">bool</span><span class="cd">  -- declare two constants at once</span>

<span class="k">#check</span> <span class="n">m</span><span class="cd">                -- output: nat</span>
<span class="k">#check</span> <span class="n">b1</span><span class="cd">               -- bool</span>
<span class="k">#check</span> <span class="n">b1</span> <span class="o">&amp;&amp;</span> <span class="n">b2</span><span class="cd">         -- "&amp;&amp;" is boolean and</span>
<span class="k">#check</span> <span class="n">b1</span> <span class="o">||</span> <span class="n">b2</span><span class="cd">         -- boolean or</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">constant</code> and <code class="language-plaintext highlighter-rouge">constants</code> commands introduce new constant symbols into the working environment. <code class="language-plaintext highlighter-rouge">#check</code> command asks Lean to report their types.</p>

<p>Lets see how we convert make new types out of others.</p>

<div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">constants</span> <span class="n">m</span> <span class="n">n</span> : <span class="n">nat</span>
<span class="k">constant</span> <span class="n">g</span> : <span class="n">nat</span> <span class="o">→</span> <span class="n">nat</span> <span class="o">→</span> <span class="n">nat</span>
<span class="k">constant</span> <span class="n">g</span><span class="err">'</span>: <span class="n">nat</span> <span class="o">→</span> (<span class="n">nat</span> <span class="o">→</span> <span class="n">nat</span>)   <span class="cd">-- has the same type as g!</span>
<span class="k">constant</span> <span class="n">f</span> : <span class="n">nat</span> <span class="o">→</span> <span class="n">nat</span><span class="cd">           -- type the arrow as "\to" or "\r"</span>
<span class="k">constant</span> <span class="n">F</span> : (<span class="n">nat</span> <span class="o">→</span> <span class="n">nat</span>) <span class="o">→</span> <span class="n">nat</span><span class="cd">   -- a "functional"</span>

<span class="k">#check</span> <span class="n">g</span> <span class="n">m</span> <span class="n">n</span><span class="cd">                     -- ℕ</span>
<span class="k">#check</span> <span class="n">g</span> <span class="n">m</span><span class="cd">                       -- ℕ → ℕ</span>
<span class="k">#check</span> <span class="n">F</span> <span class="n">f</span><span class="cd">                       -- ℕ</span>
<span class="k">#check</span> <span class="n">g</span> <span class="n">m</span> <span class="n">n</span><span class="cd">                     -- ℕ</span>
</code></pre></div></div>

<p>Thing to note from above example:</p>
<ol>
  <li>Application of a function f to a value x is denoted <code class="language-plaintext highlighter-rouge">f x</code>.</li>
  <li>Arrows associate to the right, example. <em>the type of g is nat → (nat → nat)</em>. Thus g is a function that takes natural numbers and returns another function that takes a natural number and returns a natural number.</li>
</ol>

<p>Type theory also allows for partial application of a function where, as told in point 2 above, <code class="language-plaintext highlighter-rouge">g m</code> is a function that waits for argument <code class="language-plaintext highlighter-rouge">n</code> to return <code class="language-plaintext highlighter-rouge">g m n</code>.</p>

<blockquote>
  <p><strong>Currying</strong>, redefining a function to look like other.</p>
</blockquote>

<h2 id="types-as-objects">Types as Objects</h2>

<p>Leans dependent type theory extends simple type theory by making types as object of study themselves. We can also declare new constants and constructors for types.</p>

<div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">constants</span> α β : <span class="kt">Type</span>
<span class="k">constant</span> <span class="n">F</span> : <span class="kt">Type</span> <span class="o">→</span> <span class="kt">Type</span>
<span class="k">constant</span> <span class="n">G</span> : <span class="kt">Type</span> <span class="o">→</span> <span class="kt">Type</span> <span class="o">→</span> <span class="kt">Type</span>

<span class="k">#check</span> <span class="n">nat</span><span class="cd">               -- Type</span>
<span class="k">#check</span> <span class="n">bool</span><span class="cd">              -- Type</span>
<span class="k">#check</span> <span class="n">nat</span> <span class="o">→</span> <span class="n">bool</span><span class="cd">        -- Type</span>
<span class="k">#check</span> <span class="n">nat</span> <span class="o">×</span> <span class="n">bool</span><span class="cd">        -- Type</span>
<span class="k">#check</span> α                 <span class="cd">-- Type</span>
<span class="k">#check</span> <span class="n">F</span> α               <span class="cd">-- Type</span>
<span class="k">#check</span> <span class="n">F</span> <span class="n">nat</span><span class="cd">             -- Type</span>
<span class="k">#check</span> <span class="n">G</span> α               <span class="cd">-- Type → Type</span>
<span class="k">#check</span> <span class="n">G</span> α β             <span class="cd">-- Type</span>
<span class="k">#check</span> <span class="n">G</span> α <span class="n">nat</span><span class="cd">           -- Type</span>
</code></pre></div></div>

<p>Type list α denotes the type of lists of elements of type α.</p>

<div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">constant</span> α : <span class="kt">Type</span>

<span class="k">#check</span> <span class="n">list</span> α    <span class="cd">-- Type</span>
<span class="k">#check</span> <span class="n">list</span> <span class="n">nat</span><span class="cd">  -- Type</span>
</code></pre></div></div>

<blockquote>
  <p>Lean has an infinite hierarchy of types. It’s type also has type.</p>
</blockquote>

<div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">#check</span> <span class="kt">Type</span><span class="cd">     -- Type 1</span>
<span class="k">#check</span> <span class="kt">Type</span> <span class="mi">1</span><span class="cd">   -- Type 2</span>
<span class="k">#check</span> <span class="kt">Type</span> <span class="mi">2</span><span class="cd">   -- Type 3</span>
</code></pre></div></div>

<h2 id="functions">Functions</h2>

<p>How do we create a function from another expression? We use process known as <em>abstraction</em>, or <em>lambda abstraction</em>.</p>

<p><code class="language-plaintext highlighter-rouge">x: α</code> and <code class="language-plaintext highlighter-rouge">t: β</code>. <code class="language-plaintext highlighter-rouge">fun x : α, t</code> is equivalent with <code class="language-plaintext highlighter-rouge">λ x : α, t</code>. Both are object of type α → β.</p>

<p>Example. \(f(x) = x + 5\), where \(x\) is natural number. It is translated in lean as <code class="language-plaintext highlighter-rouge">λ x : nat, x + 5</code></p>

<div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">constants</span> α β  : <span class="kt">Type</span>
<span class="k">constants</span> <span class="n">a1</span> <span class="n">a2</span> : α
<span class="k">constants</span> <span class="n">b1</span> <span class="n">b2</span> : β

<span class="k">constant</span> <span class="n">f</span> : α <span class="o">→</span> α
<span class="k">constant</span> <span class="n">g</span> : α <span class="o">→</span> β
<span class="k">constant</span> <span class="n">h</span> : α <span class="o">→</span> β <span class="o">→</span> α
<span class="k">constant</span> <span class="n">p</span> : α <span class="o">→</span> α <span class="o">→</span> <span class="n">bool</span>

<span class="k">#check</span> <span class="k">fun</span> <span class="n">x</span> : α, <span class="n">f</span> <span class="n">x</span><span class="cd">                      -- α → α</span>
<span class="k">#check</span> <span class="k">λ</span> <span class="n">x</span> : α, <span class="n">f</span> <span class="n">x</span><span class="cd">                        -- α → α</span>
<span class="k">#check</span> <span class="k">λ</span> <span class="n">x</span> : α, <span class="n">f</span> (<span class="n">f</span> <span class="n">x</span>)                    <span class="cd">-- α → α</span>
<span class="k">#check</span> <span class="k">λ</span> <span class="n">x</span> : α, <span class="n">h</span> <span class="n">x</span> <span class="n">b1</span><span class="cd">                     -- α → α</span>
<span class="k">#check</span> <span class="k">λ</span> <span class="n">y</span> : β, <span class="n">h</span> <span class="n">a1</span> <span class="n">y</span><span class="cd">                     -- β → α</span>
<span class="k">#check</span> <span class="k">λ</span> <span class="n">x</span> : α, <span class="n">p</span> (<span class="n">f</span> (<span class="n">f</span> <span class="n">x</span>)) (<span class="n">h</span> (<span class="n">f</span> <span class="n">a1</span>) <span class="n">b2</span>)  <span class="cd">-- α → bool</span>
<span class="k">#check</span> <span class="k">λ</span> <span class="n">x</span> : α, <span class="k">λ</span> <span class="n">y</span> : β, <span class="n">h</span> (<span class="n">f</span> <span class="n">x</span>) <span class="n">y</span><span class="cd">         -- α → β → α</span>
<span class="k">#check</span> <span class="k">λ</span> (<span class="n">x</span> : α) (<span class="n">y</span> : β), <span class="n">h</span> (<span class="n">f</span> <span class="n">x</span>) <span class="n">y</span><span class="cd">        -- α → β → α</span>
<span class="k">#check</span> <span class="k">λ</span> <span class="n">x</span> <span class="n">y</span>, <span class="n">h</span> (<span class="n">f</span> <span class="n">x</span>) <span class="n">y</span><span class="cd">                    -- α → β → α</span>
</code></pre></div></div>

<blockquote>
  <p>Expression <code class="language-plaintext highlighter-rouge">λ x : α, x</code> denotes the identity function on α</p>
</blockquote>

<blockquote>
  <p>We can leave type annotations on the variable, lean will infer it.</p>
</blockquote>

<p><code class="language-plaintext highlighter-rouge">λ x, g (f x)</code> == <code class="language-plaintext highlighter-rouge">λ x : α, g (f x)</code></p>

<h1 id="example">Example</h1>

<p>Here we prove that <strong>prime numbers are more than any assigned multitude of prime numbers</strong>.</p>

<blockquote>
  <p>This proposition states that there are more than any finite number of prime numbers, that is to say, there are infinitely many primes.</p>
</blockquote>

<h2 id="outline-of-proof">Outline of proof</h2>

<p><a href="https://mathcs.clarku.edu/~djoyce/java/elements/bookIX/propIX20.html">Source</a></p>

<ol>
  <li>Suppose that there are n primes, a1, a2, …, an. Euclid, as usual, takes an specific small number, n = 3, of primes to illustrate the general case. Let m be the least common multiple of all of them.
    <blockquote>
      <p>The least common multiple was also considered in proposition <a href="https://mathcs.clarku.edu/~djoyce/java/elements/bookIX/propIX14.html">IX.14</a>. It wasn’t noted in the proof of that proposition that the least common multiple of primes is their product, and it isn’t noted in this proof, either.</p>
    </blockquote>
  </li>
  <li>
    <p>Consider the number m + 1. If it’s prime, then there are at least n + 1 primes.</p>
  </li>
  <li>
    <p>So suppose m + 1 is not prime. Then according to <a href="https://mathcs.clarku.edu/~djoyce/java/elements/bookVII/propVII31.html">VII.31</a>, some prime g divides it. But g cannot be any of the primes a1, a2, …, an, since they all divide m and do not divide m + 1. Therefore, there are at least n + 1 primes. Q.E.D.</p>
  </li>
  <li>This proposition is not used in the rest of the Elements.</li>
</ol>

<h2 id="lean-proof">Lean Proof</h2>

<div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cd">-- Definitions about natural numbers and primes</span>
<span class="k">import</span> <span class="n">data</span><span class="o">.</span><span class="n">nat</span><span class="o">.</span><span class="n">prime</span><span class="cd">

-- Library on linear arithmatic</span>
<span class="k">import</span> <span class="n">tactic</span><span class="o">.</span><span class="n">linarith</span><span class="cd">

-- Define namespace, which is natural numbers in this case</span>
<span class="k">open</span> <span class="n">nat</span><span class="cd"> 


-- Define theorem or goal to prove</span>
<span class="k">theorem</span> <span class="n">infinitude_of_primes</span>: <span class="o">∀</span> <span class="n">N</span>, <span class="o">∃</span> <span class="n">p</span> <span class="o">&gt;=</span> <span class="n">N</span>, <span class="n">prime</span> <span class="n">p</span> :=<span class="cd">
-- between begin-end block we write tactics</span>
<span class="k">begin</span><span class="cd">
  -- define N to be a natural number as a part of our local hypothesis</span>
  <span class="n">intro</span> <span class="n">N</span>,

  <span class="cd">-- Continue with proof as mentioned in link provided in header
  -- let M to be N! + 1 : local definition</span>
  <span class="n">let</span> <span class="n">M</span> := <span class="n">factorial</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>,
  
  <span class="cd">-- let p be smallest prime factor of M which is not 1</span>
  <span class="n">let</span> <span class="n">p</span> := <span class="n">min_fac</span> <span class="n">M</span>,


  <span class="cd">-- define supporting hypothesis pp, p is prime</span>
  <span class="k">have</span> <span class="n">pp</span> : <span class="n">prime</span> <span class="n">p</span> :=<span class="cd"> 
  -- begin proof for supporting p being prime</span>
  <span class="k">begin</span><span class="cd">
    -- minimum factor of a number is prime, but what about if M = 1</span>
    <span class="n">refine</span> <span class="n">min_fac_prime</span> <span class="n">_</span>,
    <span class="cd">-- so here we prove M != 1 (or M &gt; 1)</span>
    <span class="k">have</span> : <span class="n">factorial</span> <span class="n">N</span> <span class="o">&gt;</span> <span class="mi">0</span> := <span class="n">factorial_pos</span> <span class="n">N</span>,
    <span class="cd">-- this just automatically takes care of linear arithmatic required for proof</span>
    <span class="n">linarith</span>,
  <span class="k">end</span>,

  <span class="cd">-- before this we had existenial statement but now we have condition in p</span>
  <span class="n">use</span> <span class="n">p</span>,

  <span class="cd">-- split our goal in  2 subgoals</span>
  <span class="n">split</span>,

  <span class="cd">-- proof by contradiction so it should output False</span>
  <span class="err">{</span><span class="n">by_contradiction</span>,
   
   <span class="o">/-</span> <span class="k">hypothesis</span> <span class="n">h1</span>, <span class="n">p</span> <span class="n">divides</span> <span class="n">N</span><span class="o">!</span> <span class="o">+</span> <span class="mi">1</span> <span class="n">proved</span> <span class="k">by</span>  
   <span class="n">min_fac_dvd</span> : <span class="o">∀</span> (<span class="n">n</span> : <span class="o">ℕ</span>), <span class="n">n</span><span class="o">.</span><span class="n">min_fac</span> <span class="err">∣</span> <span class="n">n</span>
   <span class="o">-/</span>
   <span class="k">have</span> <span class="n">h</span><span class="err">₁</span> : <span class="n">p</span> <span class="err">∣</span> <span class="n">factorial</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span> := <span class="n">min_fac_dvd</span> <span class="n">M</span>, 
   
   <span class="cd">-- hypothesis h2, p divides N!</span>
   <span class="k">have</span> <span class="n">h</span><span class="err">₂</span> : <span class="n">p</span> <span class="err">∣</span>  <span class="n">factorial</span> <span class="n">N</span> := 
   <span class="k">begin</span>
     <span class="n">refine</span> <span class="n">pp</span><span class="o">.</span><span class="n">dvd_factorial</span><span class="o">.</span><span class="n">mpr</span> <span class="n">_</span>,
     <span class="cd">-- proved p &lt;= N, using hypothsis h</span>
     <span class="n">exact</span> <span class="n">le_of_not_ge</span> <span class="n">h</span>,
   <span class="k">end</span>,
   <span class="o">/-</span>
   <span class="n">proved</span> <span class="k">using</span> <span class="n">dvd_add_right</span> <span class="k">with</span> <span class="n">support</span> <span class="k">from</span> <span class="n">local</span> <span class="k">hypothesis</span> <span class="n">h</span><span class="err">₂</span> <span class="n">and</span> <span class="n">h</span><span class="err">₁</span>
   <span class="o">-/</span>
   <span class="k">have</span> <span class="n">h</span> : <span class="n">p</span> <span class="err">∣</span> <span class="mi">1</span> := (<span class="n">nat</span><span class="o">.</span><span class="n">dvd_add_right</span> <span class="n">h</span><span class="err">₂</span>)<span class="o">.</span><span class="n">mp</span> <span class="n">h</span><span class="err">₁</span>,
   <span class="cd">-- prime not dividing one using local hypothesis pp and h</span>
   <span class="n">exact</span> <span class="n">prime</span><span class="o">.</span><span class="n">not_dvd_one</span> <span class="n">pp</span> <span class="n">h</span>, <span class="err">}</span>,
   <span class="cd">-- second part of proof is just our hypothesis pp that we already proved</span>
  <span class="err">{</span><span class="n">exact</span> <span class="n">pp</span>, <span class="err">}</span>,
<span class="k">end</span>
</code></pre></div></div>

<h1 id="references">References</h1>

<ol class="bibliography"><li><span id="Sphinx_2021_github"><i>Theorem Proving in Lean — Theorem Proving in Lean 3.23.0 Documentation</i>. <a style="color:black; text-decoration: underline;" href="https://leanprover.github.io/theorem_proving_in_lean/">https://leanprover.github.io/theorem_proving_in_lean</a>.</span></li>
<li><span id="Mathematical_Induction_2021_github"><i>Logical Verification 2020–2021</i>. <a style="color:black; text-decoration: underline;" href="https://lean-forward.github.io/logical-verification/2020/">https://lean-forward.github.io/logical-verification/2020</a>.</span></li>
<li><span id="Last_First_2021_ac"><i>The Natural Number Game</i>. <a style="color:black; text-decoration: underline;" href="https://www.ma.imperial.ac.uk/ buzzard/xena/natural_number_game/">https://ma.imperial.ac.uk/ buzzard/xenanatural_number_game</a>.</span></li>
<li><span id="Mario_Carneiro_2021_ru">Carneiro, Mario. <i>Formalizing 100 Theorems</i>. 2021, <a style="color:black; text-decoration: underline;" href="https://www.cs.ru.nl/ freek/100/index.html">https://cs.ru.nl/ freek/100/index.html</a>.</span></li>
<li><span id="Doing_2021_wordpress">Doing. <i>Xena | Mathematicians Learning Lean by Doing.</i> <a style="color:black; text-decoration: underline;" href="https://xenaproject.wordpress.com/">https://xenaproject.wordpress.com</a>.</span></li>
<li><span id="Discussing_His_Vision_2021_youtube">Vision, Discussing His. <i>The Future of Mathematics? - YouTube</i>. <a style="color:black; text-decoration: underline;" href="https://www.youtube.com/watch?v=Dp-mQ3HxgDE">https://youtube.com/watch?v=Dp-mQ3HxgDE</a>.</span></li></ol>

<!-- [Theorem proving in lean](https://leanprover.github.io/theorem_proving_in_lean/)

[lean-forward.github.io](https://lean-forward.github.io/logical-verification/2020/)

[Natural Number Game By Kevin Buzzard and Mohammad Pedramfar.](https://wwwf.imperial.ac.uk/~buzzard/xena/natural_number_game/)

[Formalizing 100 theorems](http://www.cs.ru.nl/~freek/100/index.html)

[Xena Project](https://xenaproject.wordpress.com/)

[The Future of Mathematics?](https://www.youtube.com/watch?v=Dp-mQ3HxgDE&ab_channel=MicrosoftResearch) -->
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/21/never-split-the-difference"><h3 class="contrast">Never Split the Difference</h3></a>
          <br /><span class="smaller">February 5, 2021</span>  <br />
          <div><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 100}
});
</script>

<p>Negotiation is everywhere from finalizing a time and location to meet with friends to buying a gym membership at a reasonable price. This is a self help book for anyone who wants to hone their negotation skill.</p>

<p>Negotitaion = Information Gathering + Behavior Influence</p>

<blockquote>
  <p>Great negotiation is when you win but make it seem like your counterpart won. Great negotiation is about great collaboration.</p>
</blockquote>

<h2 id="chapter-1-the-new-rules">Chapter 1: The New Rules</h2>

<p>Book starts with a very niche technique to consider during negotiation to get out of a tight spot. Below is an excerpt from the book which illustrates the technique.</p>

<div class="epigraph"><blockquote><p> “C’mon. Get me the money or I cut your son’s throat right now,” Mnookin said. Testy.
I gave him a long, slow stare. Then I smiled.
“How am I supposed to do that?” </p><footer>, <cite>Open-ended questions</cite></footer></blockquote></div>

<p>Such open-ended questioning gives your counterparts <em>illusion of control</em>. They start thinking that they possess the key to this negotiation. All this gives you one of the most important thing during negotiation <strong>the time to think</strong>.</p>

<div class="epigraph"><blockquote><p> “I’m sorry, Robert, how do I know he’s even alive?” I said, using an apology and his first name, seeding more warmth into the interaction in order to complicate his gambit to bulldoze me. “I really am sorry, but how can I get you any money right now, much less one million dollars, if I don’t even know he’s alive?”</p><footer>, <cite>Hey I have a problem, solve it for me friend!</cite></footer></blockquote></div>

<p>Presenting the problem with the terms of negotition in question form (and not straight away pointing out the issue) may compels your counterparts in negotitaion to understand the unreasonable nature of the terms.</p>

<p>Apologetic tone coupled with friendly use of words during address eases the conversation. This technique can be used when you aren’t getting any authority in negotiation. This technique will give you some chances to steer the negotitaion your way.</p>

<p>Both of the above examples correspond to tactics called <strong>calibrated questions</strong>: queries that the other side can respond to but that have no fixed answers. Main purpose of this tactics to buy you some time to think.</p>

<p>Answering such question demand that you possess deep emotional strength i.e you need to keep aside your emotions and your feeling about what other side means and from my point of view, you could even raise a counter question, like <em>what seems to be the problem with which you think I can help you</em>. This will give us insights into line of thinking of your counterpart about why he/she needs time to think through our offer or what bothers him/her. Once you get the information which your counterpart is hiding like the motive of negotitaion or even some small information which can help with your terms, you can gain authority without letting the other party know.</p>

<blockquote>
  <p>In negotiation, one should follow a strict sequence of actions (like in <a href="/articles/21/banta-zopa">Banta &amp; Zopa</a>) rather play around as per the psychology and nature of your counterpart.</p>
</blockquote>

<p>Other mistake in negotiation is to think that your counterpart is fully rational and selfish and is not affected by emotions, rather its found in studies that emotion plays a very important role in negotiation which has the power to influence the rational thinking and logical mind. <label for="thinking-fast-and-slow" class="margin-toggle sidenote-number"></label><input type="checkbox" id="thinking-fast-and-slow" class="margin-toggle" /><span class="sidenote"><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">Thinking, Fast and Slow</a>  by Daniel Kahneman. </span></p>

<p>Fisher and Ury’s approach <label for="getting-to-say-yes" class="margin-toggle sidenote-number"></label><input type="checkbox" id="getting-to-say-yes" class="margin-toggle" /><span class="sidenote">Talked about in the book <a href="https://en.wikipedia.org/wiki/Getting_to_Yes">getting to say yes</a>. </span> was introduced with acceptance of the above stated idea, and to mitigate or remove these emotions from the negotiation to collectively solve the problem in a more rational manner. The core principle of the approach was:</p>
<ol>
  <li>Seperate the emotions in negotiation</li>
  <li>Not to target the <em>what they are asking for</em> and rather look for <em>why they are asking for it</em></li>
  <li>Work in co-operation to reach a win-win deal</li>
  <li>Establish some standards for the negotiation which should be agreed by both parties</li>
</ol>

<p>Decision made by people can be influenced by \(n\) number of ways. One of which is <em>framing effect</em>. It is a proven technique showcasing that people respond differently to same question framed in different way - positive or negative connotations; e.g. as a loss or as a gain. People tend to avoid risk when a positive frame is presented but seek risks when a negative frame is presented. <label for="prospect-theory" class="margin-toggle sidenote-number"></label><input type="checkbox" id="prospect-theory" class="margin-toggle" /><span class="sidenote">Such behaviour is caused due to people being more sensitivity towards loss than for gain when things are uncertain. <a href="https://en.wikipedia.org/wiki/Loss_aversion"> [Loss Aversion]</a> but when the loss becomes more and more uncertain people tend to take more risks. </span></p>

<style>
  .embed-container {
    position: relative;
    padding-bottom:56.25%;
    padding-top:30px;
    overflow: hidden;
    max-width: 70%;
    margin-bottom: -23%;
  }
  .embed-container iframe,
  .embed-container object,
  .embed-container embed {
    position: absolute;
    top: 0;
    left: 0;
    width: 80%;
    height: 55%;
  }
</style>

<div class="embed-container">
  <iframe title="YouTube video player" width="480" height="360" src="https://www.youtube.com/embed/vBX-KulgJ1o" frameborder="0" allowfullscreen=""></iframe>
</div>

<p>Example: Bet, if I win, you give me $10 and if you win I give you $10. This will be decided with a flip of coin. Most people avert the risk and decline to play. But when same experiment is conducted with more number of trials coupled more payoff when you win, say I give you $20 but when you lose you only give $10 on each trail, people tend to take interest. This is due uncertainity of loss mathematically. \(P(heads) = 0.5\) and \(P(tails) = 0.5\). So mathematically speaking, you will win in 50% of cases and get paid with $20.</p>

<p>From the above experiment, it quite evident that people are not fully rational and rely on their gut feeling or lets say emotion to make decision. So developing a positive relationship with your counterpart is important to influence their emotions.</p>

<div class="epigraph"><blockquote><p> It all starts with the universally applicable premise that people want to be understood and accepted. Listening is the cheapest, yet most effective concession we can make to get there. </p><footer>, <cite>Active Listening</cite></footer></blockquote></div>

<p>When people think they are being listened attentively they listen to themselves in much more constructive manner lower their defenses on their point of view and become willing to accept other’s opinion. This whole technique is refered as <strong>Tactical Empathy</strong>. This technique imparts the power of influence while balancing the emotions of your counterpart during a negotiation.</p>

<h2 id="chapter-2-be-a-mirror">Chapter 2: Be a Mirror</h2>

<div class="epigraph"><blockquote><p> Good negotiators, going in, know they have to be ready for possible surprises; great negotiators aim to use their skills to reveal the surprises they are certain exist. </p><footer>, <cite>Surprise Surprise</cite></footer></blockquote></div>

<p>The moment seasoned negotiators enters a negotiation, they start forming various hypothesis about various factors affecting the negotiation. Then they use their secret weapons, i.e listening, calibrated questioning or mirroring, and narrow down the variables in negotiation. Here listening not only removes element of surprise from your counterpart’s end but also gives you enough intel to form your own arsenal of surprise.</p>

<div class="epigraph"><blockquote><p> You should engage the process with a mindset of discovery. </p><footer>, <cite>Explore then exploit</cite></footer></blockquote></div>

<p>You try forego any bias and explore uncharted territories of your counterpart’s mind as farther as you can. This is a systematic approach so never delve in any negotiation with preconcieved notions or conclusions. You can only form conclusions once you know what and who are dealing with.</p>

<div class="epigraph"><blockquote><p> Great negotiators are able to question the assumptions that the rest of the involved players accept on faith or in arrogance, and thus remain more emotionally open to all possibilities, and more intellectually agile to a fluid situation. </p><footer>, <cite>Assumptions are big no.</cite></footer></blockquote></div>

<p>You come across an assumption, next time question it right away. Extract intel, get to the your counterparts basis of assumption, what that assumption accomplish. For example. You are at a basketball court and the maintainance staff says its maintainance day. Its normal to assume that its under maintainance, but if you really want to play question them and ask if its undergoing maintainance at the very moment. If that is case, wish good bye and walk home and if not then politely ask, “can you play for sometime until they start with work?”, giving them sence of authority. You can always persist and extract more information with “How am I supposed to do that, we walked 3-5miles to come here” yada yada yada. You get my point.</p>

<div class="epigraph"><blockquote><p> until you know what you’re dealing with, you don’t know what you’re dealing with. </p><footer>, <cite>Know better</cite></footer></blockquote></div>

<p>Apart from the content and context of mind set revealed by your counterpart during your covert interogation, do not just relax with you copy pen in hand, try and match content with tone of voice, body language as they might be trying to through you off feeding you false information. You dont help the one you want to make a deal with by assuming pieces of information make his story consistent for you brain. In short. <em>Look for inconsistencies</em>.</p>

<div class="epigraph"><blockquote><p> state of schizophrenia: everyone just listening to the voice in their head. </p><footer>, <cite>Am I schizophrenic</cite></footer></blockquote></div>

<p>Its quite often the case that once you get hold of some key information which can act as pivot for your argument we go on to solidify it in our head. Essentially converting the negotiation an incomplete and futile assault towards <em>your way</em>. You dont indulge in such pedastrian tactics. You go steps and steps untill your counterpart has nothing else to give you. You dont attack early, you have secrets use then when you are certain of a win.</p>

<div class="epigraph"><blockquote><p> Wants are easy to talk about, representing the aspiration of getting our way, and sustaining any illusion of control we have as we begin to negotiate; needs imply survival, the very minimum required to make us act, and so make us vulnerable. But neither wants nor needs are where we start; it begins with listening, making it about the other people, validating their emotions, and creating enough trust and safety for a real conversation to begin. </p><footer>, <cite>I want to listen</cite></footer></blockquote></div>

<p>The above excerpt really sums up why to listen. <strong>Start with listening, breach thier defences, let them reveal, dont stop them, let them reveal, punch when you have weak points</strong>.</p>

<div class="epigraph"><blockquote><p> Going too fast is one of the mistakes all negotiators are prone to making. </p><footer>, <cite>Fast &amp; Furious</cite></footer></blockquote></div>

<p>You go on fast, you feel everything is under control. Stop! The very thing letting it be under control is the rapport you have established with your counterpart listening to him and investing time. Keep things slow and calm.</p>

<div class="epigraph"><blockquote><p> My job was to find a way to keep him talking. I switched into my Late-Night, FM DJ Voice: deep, soft, slow, and reassuring. </p><footer>, <cite>Legendary FM DJ Voice</cite></footer></blockquote></div>

<p>FM DJ Voice is a downward lilt voice. It’s best employed when establishing points of negotiation that are immovable. It is to be used in conversation no more than 20% of the time. This maintains integrity of its effect and let it convey the immovable points in arguments.</p>

<div class="epigraph"><blockquote><p> “Hey, what happened to Joe?”
I said, “Joe’s gone. This is Chris. You’re talking to me now.”
I didn’t put it like a question. I made a downward-inflecting statement, in a downward-inflecting tone of voice.
 </p><footer>, <cite>Its not up for discussion</cite></footer></blockquote></div>

<p>Above is an excerpt from the book depicting the effect of FM DJ Voice. But for major part of the conversation try to be more playful and accomodating. This helps in taking negotation forward. Try to radiate good emotions, this will make your counterpart assume that emotion. Just like when we wave and say good morning to a by stander at bus stop they replicate your emotion and action. This is how you control emotions apart from usual conversational aspects. This will get you through people of different cultures as emotions are same across the earth.</p>

<p>There is another tone called assertive. This voice is declarative, straight up, and delivered like a punch in the nose. And it converts negotiation in boxing. Dont use it until and unless absolutely necessary. It breaks more than it makes.</p>

<div class="epigraph"><blockquote><p> “The other vehicle’s not out there because you guys chased my driver away . . .” he blurted.
“We chased your driver away?” I mirrored.
“Well, when he seen the police he cut.”
“We don’t know anything about this guy; is he the one who was driving the van?” I asked.
 </p><footer>, <cite>You answer, I question.</cite></footer></blockquote></div>

<p>Mirroring is where you try to comfort your counterpart by imitating <label for="mirror-imitate" class="margin-toggle sidenote-number"></label><input type="checkbox" id="mirror-imitate" class="margin-toggle" /><span class="sidenote">Humans copy each other to comfort each other. We can copy content like using their words on them as a question or imitate their body language or tone. </span> them.</p>

<p>As per FBI, you repeat last one to three words of what someone has just said to create a mirror. This gives you superpower to seem agreeable while disagreeing to someones point.</p>

<p>Now you might be thinking how to deal with someone who is just throwing punches, talking assertive. Obviously, you can’t mirror him there. So you first of all summon your <em>Late Night FM DJ Voice</em>. Then do following:</p>

<ol>
  <li>Start with apology to comfort</li>
  <li>Mirror</li>
  <li>Be silence, 3-4 seconds to let mirror have effect</li>
  <li>Repeat</li>
</ol>

<p>Below is a small conversation on how to take on assetive person.</p>

<div class="epigraph"><blockquote><p> “Let’s make two copies of all the paperwork.” boss said<br />
“I’m sorry, two copies?” she mirrored in FM DJ voice + inquisitive tone (meaning to ask for help)<br />
“Yes,” her boss responded, “one for us and one for the customer.” <br />
“I’m sorry, so you are saying that the client is asking for a copy and we need a copy for internal use?” She asked<br />
“Actually, I’ll check with the client—they haven’t asked for anything. But I definitely want a copy. That’s just how I do business.” boss replied<br />
“Absolutely,” she responded. “Thanks for checking with the customer. Where would you like to store the in-house copy? There’s no more space in the file room here.” She asked <br />
“It’s fine. You can store it anywhere,” he said, slightly perturbed now.<br />
“Anywhere?” she mirrored again, with calm concern.<br />
“As a matter of fact, you can put them in my office,” he said, with more composure than he’d had the whole conversation.<br />
“I’ll get the new assistant to print it for me after the project is done. For now, just create two digital backups.” Boss said </p><footer>, <cite>Sorry but I will mirror</cite></footer></blockquote></div>

<p>Use mirrors to create a bond, keep people talking, buy time, and encourage your counterparts to reveal their strategy.</p>

<!-- ## Chapter 3: Don’t Feel Their Pain, Label It -->
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/21/banta-zopa"><h3 class="contrast">BANTA &amp; ZOPA</h3></a>
          <br /><span class="smaller">February 2, 2021</span>  <br />
          <div><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 100}
});
</script>

<h2 id="banta">Banta</h2>

<p>“BATNA” is an acronym which stands for ‘Best Alternative to a Negotiated Agreement. It helps you in making a guess about your options if you are unsuccessful in agreeing on a deal.</p>

<p>During negotiation, you not only need to access your BANTA but also of your negotiation counterparts. If your counterpart doesn’t have options then your BANTA doesn’t hold too much significance and same goes for you.</p>

<p>Apart from establishing a BANTA, it is also advised to establish a <strong>Reservation Value</strong>, which is least favourable number on which you will agree.</p>

<p>Example: If you are in a condition where you already have a initial offer on a piece of hardware, say $1900, then such offer can act as BANTA as long as you are comfortable with it and willing to accept it. Then you might want to set a bit higher reservation value, let say $2000, for your other negotitions.</p>

<h2 id="zopa">Zopa</h2>

<p>"”ZOPA” stands for Zone of Possible Agreement. As the name suggests, it is range which both parties consider favourable as per their established BANTA.</p>

<figure><figcaption><a href="http://www.successfulnegotiators.com/negotiators-blog/2017/1/16/basic-negotiation-terminology-batna-reservation-value-zopa">Source</a></figcaption><img src="/assets/img/banta-zopa/banta-zopa.png" /></figure>

<p>We generally are more interested in knowing our counterpart’s BANTA and reservation value. It is found out which some certainity during the negotition. But before the negotiation, we need to collect some open information like if you can get his/her bid for similar item, etc. Then using such information, we make our initial estimate.</p>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/21/thinking-fast-and-slow"><h3 class="contrast">Thinking, Fast and Slow</h3></a>
          <br /><span class="smaller">January 31, 2021</span>  <br />
          <div><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 100}
});
</script>

<p>The book revolves around two systems that drive human thought:</p>

<ul>
  <li>System 1: Fast, automatic, intuitive, and emotional. It operates effortlessly and is prone to biases and errors due to its reliance on heuristics.</li>
  <li>System 2: Slow, deliberate, analytical, and logical. It requires effort and is often engaged for complex or unfamiliar problems.</li>
</ul>

<p>Example: System 1 helps us quickly judge if someone looks angry, while System 2 is needed to solve a math problem like 17 × 24.</p>

<h2 id="anchoring-effect">Anchoring Effect</h2>

<p>Our judgments are often influenced by arbitrary numbers or information we encounter, even when they are irrelevant to the task at hand. This is the anchoring effect.</p>

<p>Example: When asked to estimate the percentage of African nations in the UN, participants’ answers were influenced by spinning a wheel of fortune showing random numbers. A higher number led to higher estimates.</p>

<p>Anchoring occurs when individuals rely too heavily on the first piece of information they encounter</p>

<h2 id="the-availability-heuristic">The Availability Heuristic</h2>

<p>People judge the probability of an event based on how easily examples come to mind. This can lead to overestimating the likelihood of dramatic or recent events.</p>

<p>Example: Plane crashes are perceived as more frequent than they are because they receive extensive media coverage, making them easier to recall compared to car accidents, which are far more common.</p>

<h2 id="loss-aversion">Loss Aversion</h2>

<p>Humans feel the pain of loss more acutely than the pleasure of gain. This principle explains why people often avoid risks, even when potential rewards outweigh the risks.</p>

<p>Example: Losing $50 feels more painful than the pleasure gained from winning $50.</p>

<p>Loss aversion is a key concept in prospect theory, which explains decision-making under risk.</p>

<h2 id="the-endowment-effect">The Endowment Effect</h2>

<p>Ownership increases the perceived value of an object. People often demand much more to give up an item they own than they would be willing to pay to acquire it.</p>

<p>Example: A person who owns a mug might value it at $10, but if they didn’t own it, they might only be willing to pay $5 for it.</p>

<h2 id="overconfidence-bias">Overconfidence Bias</h2>

<p>People tend to overestimate the accuracy of their judgments and knowledge. This can lead to poor decision-making, especially in complex situations where uncertainty is high.</p>

<p>Example: Experts in fields like stock market prediction often overrate their ability to forecast trends accurately.</p>

<h2 id="the-planning-fallacy">The Planning Fallacy</h2>

<p>People are overly optimistic about how much time, resources, or effort a task will take, often underestimating potential obstacles.</p>

<p>Example: Construction projects frequently take longer and cost more than initially planned due to this bias.</p>

<p>The planning fallacy is a tendency to underestimate time, costs, and risks, while overestimating benefits.</p>

<h2 id="the-halo-effect">The Halo Effect</h2>

<p>First impressions or a single positive trait can influence how we perceive unrelated attributes of a person or situation.</p>

<p>Example: A competent and friendly employee might be assumed to be more capable than they actually are, just because of their warmth.</p>

<h2 id="regression-to-the-mean">Regression to the Mean</h2>

<p>When outcomes are extreme, they tend to be followed by more moderate outcomes. This is not due to causation but simply statistical probability.</p>

<p>Example: A student who scores unusually high on a test is likely to score closer to average on subsequent tests.</p>

<p>These excerpts illustrate the rich insights from Thinking, Fast and Slow and highlight its relevance to understanding decision-making, biases, and human behavior. Let me know if you’d like to explore specific chapters or concepts further!</p>

</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/21/a-test-draft"><h3 class="contrast">Lean Proof Assistant</h3></a>
          <br /><span class="smaller">January 21, 2021</span>  <br />
          <div>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/21/how-to-read-head-ct"><h3 class="contrast">How to read HeadCT</h3></a>
          <br /><span class="smaller">January 9, 2021</span>  <br />
          <div><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 100}
});
</script>

<p>This blogs talks about how to read head ct scans and how to identify different types of bleeds.</p>

<h1 id="introduction">Introduction</h1>

<p>Head CT is also called Brain CT. It refers to computed tomography examination of the brain and surrounding structures. It can be performed in 2 ways:</p>
<ol>
  <li>Non-Contrast Study</li>
  <li>Contrast Study</li>
</ol>

<p><strong>Preparation:</strong></p>

<p>Without Contrast: No preparation is required.</p>

<p>With Contrast:</p>
<ol>
  <li>Only one CT contrast study should be scheduled within a 48 hour period.</li>
  <li>BUN &amp; Creatinine must be done within 72 hours of the scan.</li>
  <li>Nothing but clear liquid after midnight before the scan.</li>
  <li>NPO 4 hours prior to exam (no food or drink).</li>
</ol>

<p>During examination of CT scans, it is important to review old scans as they may provide information which is as clinically important as the current scans</p>

<p><strong>For example</strong>:</p>

<figure><figcaption>Example of how past cases may influence diagnosis and treatment.</figcaption><img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/pathology/previous.jpg" /></figure>
<p><img src="" alt="" /></p>

<blockquote>
  <p>This elderly patient - who presented with acute confusion - has a large area of low density in the right frontal lobe seen on the current CT Review of a previous CT revealed that the abnormality in the brain was not new, but related to an infarct which occurred 5 months earlier The current CT appearances were not the cause of the acute confusion (urinary tract infection in this case)</p>
</blockquote>

<p><a href="https://wwww.radiologymasterclass.co.uk/tutorials/ct/ct_acute_brain/ct_brain_details">Source</a></p>

<h1 id="approach">Approach</h1>

<p>In emergency situations radiologists tend to follow this mnemonic: <strong>Blood Can Be Very Bad</strong> (BCBVB).</p>

<p>B: blood</p>

<blockquote>
  <p>look for epidural hematoma, subdural hematoma, intraparenchymal hemorrhage, intraventricular hemorrhage, subarachnoid hemorrhage and (also) extracranial hemorrhage</p>
</blockquote>

<p>C: cisterns</p>

<blockquote>
  <p>look for the presence of blood, effacement and asymmetry in four key cisterns (perimesencephalic, suprasellar, quadrigeminal and Sylvian cisterns)</p>
</blockquote>

<p>B: brain</p>

<blockquote>
  <p>look for asymmetry or effacement of the sulcal pattern, gray-white matter differentiation (including the insular ribbon sign), structural shifts and abnormal hypodensities (e.g. air, edema, fat) or hyperdensities (e.g. blood, calcification)</p>
</blockquote>

<p>V: ventricles</p>

<blockquote>
  <p>look for intraventricular hemorrhage, ventricular effacement or shift and for hydrocephalus</p>
</blockquote>

<p>B: bone</p>

<blockquote>
  <p>look for skull fractures (especially basal) on bone windows (soft tissue swelling, mastoid air cells and paranasal sinuses fluid in the setting of trauma should raise the possibility of a skull fracture; intracranial air means that the skull and the dura have been violated somewhere)</p>
</blockquote>

<p><a href="https://radiopaedia.org/articles/emergency-ct-head-mnemonic?lang=us">Source</a></p>

<p>To look for these neuropathology, there is a 3 step process:</p>
<figure>
<img src="/assets/img/read-ct/step1.png" width="400" />
<figcaption>Step 1</figcaption>
</figure>
<figure>
<img src="/assets/img/read-ct/step2.png" width="400" />
<figcaption>Step 2</figcaption>
</figure>
<figure>
<img src="/assets/img/read-ct/step3.png" width="400" />
<figcaption>Step 3</figcaption>
</figure>

<p><strong>For this blog, we focus on bleeds/hemorrhage and its subtypes.</strong></p>

<p>The following points can be used as a guide to assess a brain CT to demonstrate/exclude a hemorrhage.</p>

<p><strong>Brain parenchyma</strong></p>
<ul>
  <li>is there asymmetry anywhere or obliteration of the gyri sulci pattern?</li>
  <li>abnormal gray-white matter differentiation?</li>
  <li>hypo/hyperdense abnormalities?</li>
</ul>

<p><strong>Hemorrhage</strong></p>
<ul>
  <li>type/cause/location?</li>
  <li>subarachnoid cisterns; obliteration of the W shape, pentagon, moon shape, Sylvian fissure?</li>
  <li>mass effect or signs of herniation?  is there still space around the brain stem?</li>
</ul>

<p><strong>Ventricular system</strong></p>
<ul>
  <li>hydrocephalus?</li>
  <li>intraventricular blood?</li>
</ul>

<p><strong>Bone</strong></p>
<ul>
  <li>extracranial soft tissue swelling?</li>
  <li>fracture? Pneumocephalus?</li>
  <li>normal air content of the sinuses and the mastoid?  Air-fluid (blood) levels in the sinus? (CAUTION: fracture!)</li>
</ul>

<p><strong>Old examinations</strong></p>
<ul>
  <li>new findings?</li>
</ul>

<h1 id="hemorrhage-categorization">Hemorrhage Categorization</h1>

<p>When a blood vessel within the skull is ruptured or leaks, it is called <strong>Intracranial hemorrhage (ICH)</strong></p>

<p>ICH encompasses many conditions characterized by the extravascular accumulation of blood within different intracranial spaces. Following categorization is based on locaiton:</p>

<blockquote>
  <p><strong>Intra-axial hemorrhage</strong> is bleeding within the brain itself, or cerebral hemorrhage. This category includes intraparenchymal hemorrhage (bleeding within the brain tissue) and intraventricular hemorrhage (bleeding within the brain’s ventricles).</p>
</blockquote>

<p>Subtypes:</p>

<p><strong>intracerebral hemorrhage</strong></p>
<ul>
  <li>lobar hemorrhage</li>
  <li>hypertensive hemorrhage
    <ul>
      <li>basal ganglia hemorrhage</li>
      <li>pontine hemorrhage</li>
      <li>cerebellar hemorrhage</li>
    </ul>
  </li>
</ul>

<p>Intracerebral hemorrhage also called intraparenchymal cerebral hemorrhage, is a subset of an intracranial hemorrhage. It is the acute accumulation of blood within the parenchyma (tissues) of the brain.</p>

<blockquote>
  <p><strong>Extra-axial hemorrhage</strong>, bleeding that occurs within the skull but outside of the brain tissue, falls into three subtypes: Epidural hemorrhage (extradural hemorrhage) which occur between the dura mater (the outermost meninx) and the skull, is caused by trauma.</p>
</blockquote>

<p>Subtypes:</p>
<ul>
  <li>extradural hemorrhage (EDH)</li>
  <li>intralaminar dural hemorrhage</li>
  <li>subdural hemorrhage (SDH)</li>
  <li>subarachnoid hemorrhage (SAH)</li>
  <li>intraventricular hemorrhage (IVH)</li>
  <li>subpial hemorrhage</li>
</ul>

<p>Lets look at above mentioned neuropathology based on 3 steps metioned in Introduction:</p>

<h2 id="brain-parenchyma">Brain Parenchyma</h2>

<blockquote>
  <p>Intra-axial hemorrhages</p>
</blockquote>

<h3 id="lobar-hemorrhage">Lobar hemorrhage</h3>

<p>Location : <code class="language-plaintext highlighter-rouge">Lobar -&gt; Lobes</code></p>

<p><strong>Hyperdense collection of blood</strong>, located superficially within the lobes of the brain (i.e. not in the basal ganglia).</p>

<p>They are usually large and more common in elderly patients.</p>

<figure>
<img src="/assets/img/read-ct/lobar-bleed.png" width="400" />
<figcaption>Case courtesy of Assoc Prof Frank Gaillard, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/10678">rID: 10678</a></figcaption>
</figure>

<p>Extension into the subdural or subarachnoid and even intraventricular(common to basal ganglia hemorrhage - <strong>hypertensive</strong>) space may be seen.</p>

<h3 id="hypertensive-hemorrhage">Hypertensive hemorrhage</h3>

<p>Distribution that matches incidence of hypertensive hemorrhages:</p>
<ul>
  <li>80% around basal ganglia</li>
  <li>10% pons</li>
  <li>10% cerebellum</li>
</ul>

<figure>
<img src="/assets/img/read-ct/hypertensive-dist.png" width="600" />
<figcaption>Hypertensive hemorrhages</figcaption>
</figure>

<p>1- <strong>Basal ganglia hemorrhage</strong></p>

<p>Name pretty much gives away the location of hemorrhage: around basal ganglia.</p>

<p><strong>Abnormal Hyperdensity</strong> centered on the basal ganglia or thalamus. It will be characterized as intraparenchymal hemorrhage.</p>

<figure>
<img src="/assets/img/read-ct/basal-ganglia-bleed.png" width="400" />
<figcaption>Case courtesy of Assoc Prof Frank Gaillard, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/2764">rID: 2764</a></figcaption>
</figure>

<p>Often times there may be an intraventricular extension. In this case it also becomes intraventricular hemorrhage.</p>

<figure>
<img src="/assets/img/read-ct/basal-ganglia-bleed-extension.png" width="400" />
<figcaption>Case courtesy of Dr Jeremy Jones, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/6223">rID: 6223</a></figcaption>
</figure>

<p>2- <strong>Pontine hemorrhage</strong></p>

<p>location: <code class="language-plaintext highlighter-rouge">pontine -&gt; pons</code></p>

<p>These bleeds frequently rupture into the 4th ventricle as shown in figure.</p>

<figure>
<img src="/assets/img/read-ct/pontine-bleed.png" width="400" />
<figcaption>Case courtesy of Assoc Prof Frank Gaillard, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/11062">rID: 11062</a></figcaption>
</figure>

<p>3- <strong>Cerebellar hemorrhage</strong></p>

<p>location: <code class="language-plaintext highlighter-rouge">cerebellar hemispheres</code></p>

<p>It appears as hyperdensity within the cerebellar hemispheres. Extension into the fourth ventricle or subarachnoid space is relatively common.</p>

<figure>
<img src="/assets/img/read-ct/cerebellar-bleed.png" width="400" />
<figcaption>Case courtesy of Dr Farzad Pirzad, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/9620">rID: 9620</a></figcaption>
</figure>

<h2 id="outside-brain-parenchyma">Outside Brain Parenchyma</h2>

<blockquote>
  <p>Extra-axial hemorrhage</p>
</blockquote>

<h3 id="extradural-hemorrhage">Extradural hemorrhage</h3>

<p>location: <code class="language-plaintext highlighter-rouge">between the inner surface of the skull and outer layer of the dura</code></p>

<p>shape: <code class="language-plaintext highlighter-rouge">bi-convex (or lentiform)</code></p>

<figure>
<img src="/assets/img/read-ct/diagram-intracranial-haemorrhage.jpg" width="600" />
<figcaption>Case courtesy of Dr Matt Skalski, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/21542">rID: 21542</a></figcaption>
</figure>

<p>Extradural hematoma (EDH), also known as an epidural hematoma.</p>

<p>EDHs are <strong>hyperdense</strong>. Depending on their size, secondary features of mass effect (e.g. midline shift, subfalcine herniation, uncal herniation) may be present.</p>

<figure>
<img src="/assets/img/read-ct/extradural-bleed.png" width="400" />
<figcaption>Case courtesy of Dr Sandeep Bhuta , <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/4458">rID: 4458</a></figcaption>
</figure>

<h3 id="intralaminar-dural-hemorrhage">Intralaminar dural hemorrhage</h3>

<p>location: <code class="language-plaintext highlighter-rouge">between the two layers of the dura mater</code></p>

<p>Intralaminar dural hematomas in the intracranial space, are exceedingly rare.</p>

<h3 id="subdural-hemorrhage">Subdural hemorrhage</h3>

<p>location: <code class="language-plaintext highlighter-rouge">between the dura and arachnoid mater of the meninges around the brain</code></p>

<figure>
<img src="/assets/img/read-ct/diagram-subdural-haemorrhage.jpg" width="600" />
<figcaption>Case courtesy of Dr Matt Skalski, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/21542">rID: 21542</a></figcaption>
</figure>

<p>The aspect of a Subdural hemorrhage (SDH) on a CT may vary: from hyperdense/heterogeneous in the acute phase to iso/hypodense during the chronic phase. In a mixed picture, fresh hemorrhages are seen in a chronic subdural hematoma.</p>

<p><strong>Hyperacute</strong></p>

<p>appearance: <code class="language-plaintext highlighter-rouge">relatively isodense (same density) to cortex with swirled appearance</code>.</p>

<p>mass-effect: <code class="language-plaintext highlighter-rouge">cerebral swelling accentuates the mass-effect created by the collection of the clot, serum and ongoing unclotted blood.</code></p>

<p><strong>Acute</strong></p>

<p>shape: <code class="language-plaintext highlighter-rouge">crescent-shaped</code></p>

<p>appearance: <code class="language-plaintext highlighter-rouge">hyperdense relative to the cortex</code></p>

<p><strong>Subacute</strong></p>

<p>duration: <code class="language-plaintext highlighter-rouge">takes 3-21 days</code></p>

<p>appearance: <code class="language-plaintext highlighter-rouge">isodense to the adjacent cortex</code></p>

<blockquote>
  <p>Isodense appearance makes it hard to detect</p>
</blockquote>

<p>key-id’s : <code class="language-plaintext highlighter-rouge">mass-effect, thickening of cortex, CSF-filled sulci do not reach the skull but rather fade out into the subdural.</code></p>

<p><strong>Chronic</strong></p>

<p>duration: <code class="language-plaintext highlighter-rouge">~3 weeks</code></p>

<p>shape: <code class="language-plaintext highlighter-rouge">crescentic shape may change to a biconvex</code></p>

<p>appearance: <code class="language-plaintext highlighter-rouge">hypodense</code></p>

<figure>
<img src="/assets/img/read-ct/subdural-acute-chronic.png" width="500" />
<figcaption>Chronic subdural hematoma (= hypodense) at right with an acute bleeding component (= hyperdense). <a href="https://www.startradiology.com/internships/neurology/brain/ct-brain-hemorrhage/">Source</a></figcaption>
</figure>

<p>Please refer to ct scans for all above stages <a href="https://radiopaedia.org/articles/subdural-haemorrhage?lang=us">here</a>.</p>

<h3 id="subarachnoid-hemorrhage-sah">Subarachnoid hemorrhage (SAH)</h3>

<p>location: <code class="language-plaintext highlighter-rouge">subarachnoid spaces</code></p>

<blockquote>
  <p>The subarachnoid spaces include the basal cisterns (= space around the brain stem), the Sylvian fissure, the cerebral sulci, the intraventricular space and the interhemispheric fissure</p>
</blockquote>

<figure>
<img src="/assets/img/read-ct/diagram-subarachnoid-haemorrhage.jpg" width="600" />
<figcaption>Case courtesy of Dr Matt Skalski, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/21542">rID: 21542</a></figcaption>
</figure>

<p>Small amounts of blood can sometimes be appreciated pooling in the interpeduncular fossa, appearing as a small hyperdense triangle, or within the occipital horns of the lateral ventricles.</p>

<figure>
<img src="/assets/img/read-ct/subarachnoid-prepontine-cisterns-bleed.png" width="500" />
<figcaption>Subarachnoid blood in the prepontine cisterns (hyperdense obliteration of the moon shape).<a href="https://www.startradiology.com/internships/neurology/brain/ct-brain-hemorrhage/">Source</a></figcaption>
</figure>

<figure>
<img src="/assets/img/read-ct/subarachnoid-cortical.png" width="500" />
<figcaption>Blood along the right cerebral convexity. The blood follows the cortical gyri sulci pattern, characteristic of subarachnoid blood.<a href="https://www.startradiology.com/internships/neurology/brain/ct-brain-hemorrhage/">Source</a></figcaption>
</figure>

<figure>
<img src="/assets/img/read-ct/subarachnoid-intraventricular.png" width="500" />
<figcaption>Extensive intraventricular blood in the left lateral ventricle, the aqueduct and the 4th ventricle.<a href="https://www.startradiology.com/internships/neurology/brain/ct-brain-hemorrhage/">Source</a></figcaption>
</figure>

<h3 id="intraventricular-hemorrhage">Intraventricular hemorrhage</h3>

<p>Intraventricular hemorrhage (IVH) denotes the presence of blood within the cerebral ventricular system.</p>

<p>IVH is divided in 2 types: <label for="side-note-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="side-note-3" class="margin-toggle" /><span class="sidenote">\(P(primary) &lt; P(secondary) \rightarrow\) P being probablity </span></p>
<ol>
  <li>Primary - blood in the ventricles with little (if any) parenchymal blood</li>
  <li>Secondary - a large extraventricular component is present (e.g. parenchymal or subarachnoid) with secondary extension into the ventricles</li>
</ol>

<p>In above sections on hypertensive hemorrhage and subarachnoid hemorrhage, IVH CT scan is shown.</p>

<h3 id="subpial-hemorrhage">Subpial hemorrhage</h3>

<p>location: <code class="language-plaintext highlighter-rouge">between the cortical surface and the pia mater</code></p>

<blockquote>
  <p>Difficult to distinguish from subarachnoid hemorrhage.</p>
</blockquote>

<p>It is very rare type of extra-axial intracranial hemorrhage.</p>

<h2 id="cerebral-hemorrhagic-contusion">Cerebral hemorrhagic contusion</h2>

<p>location: <code class="language-plaintext highlighter-rouge">in the frontal lobes adjacent to the floor of the anterior cranial fossa and in the temporal poles</code>.</p>

<p>appearance: <code class="language-plaintext highlighter-rouge">hyperdense foci</code></p>

<figure>
<img src="/assets/img/read-ct/cerebral-contusions-bifrontal.png" width="400" />
<figcaption>Case courtesy of Assoc Prof Frank Gaillard, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/7149">rID: 7149</a></figcaption>
</figure>

<p>Hemorrhagic contusion are common to see in significant head injury. Most contusions represent the brain coming to a sudden stop against the inner surface of the skull (contrecoup) accentuated by the natural contours of the skull.</p>

<blockquote>
  <p>Contusions vary in size and can appear as small petechial foci of hyperdensity/hemorrhages involving the grey matter and subcortical white matter or large cortical/subcortical bleed.</p>
</blockquote>

<h1 id="key-findings">Key Findings</h1>

<p><strong>Midline Shift</strong></p>

<p>Midline of the brain is curved due to pressure build up inside brain either due to swelling or bleeding.</p>

<figure>
<img src="/assets/img/read-ct/midline.jpg" width="400" />
<figcaption>Case courtesy of UoE Radiology, <a href="https://radiopaedia.org/">Radiopaedia.org</a>. From the case <a href="https://radiopaedia.org/cases/34098">rID: 34098</a></figcaption>
</figure>

<p>Caused due to following:</p>
<ol>
  <li>extra-axial collection causing mass effect</li>
  <li>parenchymal tumor causing mass effect</li>
  <li>stroke with associated edema causing mass effect</li>
</ol>

<p><strong>Mass Effect</strong></p>

<p>Mass effect describes what happens around a tumor in the brain. It may be caused by:</p>
<ol>
  <li>tumors</li>
  <li>cerebral abscess</li>
  <li>infarction and associated edema</li>
  <li>hemorrhage</li>
</ol>

<p><strong>Hydrocephalus</strong></p>

<p>It describes the situation where the intracranial ventricular system is enlarged because of increased pressure. It may be caused by:</p>
<ol>
  <li>obstruction of CSF flow.</li>
  <li>altered CSF dynamics.</li>
</ol>

</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/21/ct-brain-anatomy"><h3 class="contrast">Brain Anatomy using CT Scans</h3></a>
          <br /><span class="smaller">January 5, 2021</span>  <br />
          <div><blockquote>
  <p>What is a CT scan? Brain components visible in CT scan and their relevance.</p>
</blockquote>

<p>This blog doesn’t dig deep into anatomy and may miss many key terms. Its purpose is to be familiar with brain anatomy for CT.</p>

<p>CT scans use a series of X-ray beams passed through the head. The images are then developed on sensitive film. This method creates cross-sectional images of the brain and shows the structure of the brain, but not its function.</p>

<p>Brain imaging methods allow neuroscientists to see inside the living brain. These methods help neuroscientists:</p>
<ul>
  <li>Understand the relationships between specific areas of the brain and what function they serve.</li>
  <li>Locate the areas of the brain that are affected by neurological disorders.</li>
  <li>Develop new strategies to treat brain disorders</li>
</ul>

<p><label for="side-note-video" class="margin-toggle"> ⊕</label><input type="checkbox" id="side-note-video" class="margin-toggle" /><span class="marginnote">Brain Imaging, Crash Course </span></p>
<style>
  .embed-container {
    position: relative;
    padding-bottom:56.25%;
    padding-top:30px;
    overflow: hidden;
    max-width: 70%;
    margin-bottom: -23%;
  }
  .embed-container iframe,
  .embed-container object,
  .embed-container embed {
    position: absolute;
    top: 0;
    left: 0;
    width: 80%;
    height: 55%;
  }
</style>

<div class="embed-container">
  <iframe title="YouTube video player" width="480" height="360" src="https://www.youtube.com/embed/DdPRfPm9SI4" frameborder="0" allowfullscreen=""></iframe>
</div>

<h1 id="introduction">Introduction</h1>

<p>Understanding brain anatomy is important to read CT scans. In this blog we will go through key anatomy which will make CT scans more apparent to you. For the first, CT scans are viewed from below, so right side of the brain is on left side of viewer. Front part of head (Anterior) is on top.</p>

<figure>
<img src="/assets/img/brain-anatomy-ct/sides.png" width="400" />
<figcaption>CT Scan Orientation</figcaption>
</figure>

<h1 id="anatomy">Anatomy</h1>

<h2 id="skull">Skull</h2>

<p>The brain is located inside an area bounded by skull and skull base called cranial vault. It is a protective casing for brain and brianstem. At the skull base the bones of the cranial vault form the cranial fossae which accommodate and support the brain.</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/6/69/1311_Brain_Stem.jpg" width="400" /> <label for="note-id" class="margin-toggle"> ⊕</label><input type="checkbox" id="note-id" class="margin-toggle" /><span class="marginnote">Brain Stem </span></p>

<p><img src="/assets/img/brain-anatomy-ct/cranial-fossae.png" width="300" /><label for="note-id" class="margin-toggle"> ⊕</label><input type="checkbox" id="note-id" class="margin-toggle" /><span class="marginnote">Cranial Fossae </span></p>

<p>Skull sutures has jagged appearance by nature they are not to be confused with fracture. In the figure, <strong>dipole</strong> is a porous bone tissue which seprates outer (thick and dense) and inner (thin, dense and brittle) tables. <a href="https://en.wikipedia.org/wiki/Calvaria_(skull)#Layers">Source</a></p>

<figure>
<img src="/assets/img/brain-anatomy-ct/skull.png" width="400" />
<figcaption>Skull Bone Structure</figcaption>
</figure>

<p><strong>Sutures</strong>: There are 4 main sutures:</p>
<ol>
  <li>Coronal (Connects <em>frontal bone with parietal bones</em>)</li>
  <li>Sagittal (Connects <em>parietal bones in the midline</em>)</li>
  <li>Lambdoid (Connects <em>parietal bones with the occipital bone</em>)</li>
  <li>Squamosal (Connects <em>squamous portion of the temporal bone with the parietal bones</em>)</li>
  <li>Metopic (Present in adults, Connects <em>2 fontal bones</em>)</li>
</ol>

<figure>
<img src="/assets/img/brain-anatomy-ct/sutures.png" width="400" />
<figcaption>Sutures</figcaption>
</figure>

<blockquote>
  <p>Thinnest part of skull is <strong>PTERION</strong>. The frontal, parietal, temporal and sphenoid bones unite at this point.</p>
</blockquote>

<h2 id="sinuses">Sinuses</h2>

<p>The sinuses are hollow spaces in the skull and the face bones around your nose.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Sinuses Overlay on Face</th>
      <th style="text-align: center">Sinuses Overlay on CT</th>
      <th style="text-align: center">Frontal Sinuses on CT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/assets/img/brain-anatomy-ct/sinuses-face.png" width="400" /></td>
      <td style="text-align: center"><img src="/assets/img/brain-anatomy-ct/sinuses.png" width="300" /></td>
      <td style="text-align: center"><img src="/assets/img/brain-anatomy-ct/frontal-sinuses.png" width="300" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>The mastoid air cells are continuous with the middle ear.</p>
</blockquote>

<blockquote>
  <p>In case of trauma, fluid in the sphenoid sinus may be a sign of a basal skull fracture.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: center">CT Scan for Basal skull fracture</th>
      <th style="text-align: center">Symptoms</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/assets/img/brain-anatomy-ct/basilar-skull.png" width="330" /></td>
      <td style="text-align: center"><img src="/assets/img/brain-anatomy-ct/skull-fracture-signs.png" width="350" /></td>
    </tr>
  </tbody>
</table>

<p><a href="http://www.emdocs.net/basilar-skull-fracture-basics-beyond/">Source</a></p>

<h2 id="meninges">Meninges</h2>

<p>Meninges refer to the membranous coverings of the brain and spinal cord.</p>

<p>Knowledge about meninges is essential for understanding intracranial bleeding and infection (meningitis) in CT scans.</p>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/meninges_4.jpg" />
<figcaption>Meninges</figcaption>
</figure>

<p>There are three layers of meninges (Acronym: Digital Audio Player):</p>
<ul>
  <li>Dura mater</li>
  <li>Arachnoid mater</li>
  <li>Pia mater.</li>
</ul>

<figure>
<img src="/assets/img/brain-anatomy-ct/Overview-of-the-Meninges-of-the-Brain.png" width="400" />
<figcaption>Overview of the meninges, and their relationship to the skull and brain</figcaption>
</figure>

<p>Functions of these covering include:</p>
<ul>
  <li>To act as supportive framework for the cerebral and cranial vasculature.</li>
  <li>Acting with cerebrospinal fluid to protect the central nervous system from mechanical damage.</li>
</ul>

<h3 id="dura-mater">Dura Mater</h3>
<p><a href="https://teachmeanatomy.info/neuroanatomy/structures/meninges/">Source</a></p>

<p>Physical property: hick, tough and inextensible.</p>

<p>Within the cranial cavity, the dura contains two connective tissue sheets:</p>
<ul>
  <li>Periosteal layer – lines the inner surface of the bones of the cranium.</li>
  <li>Meningeal layer – only layer present in the vertebral column.</li>
</ul>

<p><a href="https://teachmeanatomy.info/wp-content/uploads/Dura-Mater-in-Meningitis.jpg"><strong>Viewer Discretion is advised</strong> - Autopsy of a patient with meningitis. The dura mater is being retracted to show a grossly swollen cerebrum with pus accumulation.</a></p>

<p><strong>Tentorium cerebelli</strong></p>

<p>Tentorium is an extension of the dura; it separates the cerebrum (brain) from the cerebellum.</p>

<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/b/b7/Illu_tentorium.jpg" />
</figure>

<figure>
<img src="/assets/img/brain-anatomy-ct/bleed.png" width="400" />
<figcaption>Tentorial Subdural Hemorrhage</figcaption>
</figure>

<p><a href="https://sinaiem.org/tentorial-subdural-hemorrhage/#:~:text=They%20may%20resemble%20intraparenchymal%20bleed,portion%20of%20the%20occipital%20lobes.">Source</a></p>

<p>This is what Tentorial Subdural Hemorrhage look like on CT. It may resemble <a href="https://en.wikipedia.org/wiki/Intraparenchymal_hemorrhage#:~:text=Intraparenchymal%20hemorrhage%20(IPH)%20is%20one,a%20wide%20spectrum%20of%20disorders.">intraparenchymal bleed</a> on CT scans in rare cases.</p>

<h3 id="arachnoid-mater">Arachnoid Mater</h3>

<p>Underneath the arachnoid is a space known as the sub-arachnoid space. It contains cerebrospinal fluid, which acts to cushion the brain.</p>

<h3 id="pia-mater">Pia Mater</h3>

<p>Physical Property: very thin, and tightly adhered to the surface of the brain and spinal cord.</p>

<p>It covers contours of the brain (the gyri and fissures).</p>

<h3 id="falx-cerebri">Falx cerebri</h3>

<p>It lies in midline and separates the left and right cerebral hemispheres.</p>

<figure>
<img src="/assets/img/brain-anatomy-ct/flax.png" width="400" />
<figcaption>Falx cerebri</figcaption>
</figure>

<p>A midline shift may occur when the pressure exerted by the buildup of blood and swelling around the damaged brain tissues is powerful enough to push the entire brain off-center.</p>

<figure>
<img src="https://prod-images-static.radiopaedia.org/images/13472872/cb1f533317efe6888fc9620f217ffe_gallery.jpg" width="400" />
<figcaption>It suggestes bleeds or swelling in brain.</figcaption>
</figure>

<h2 id="cerebrospinal-fluid-csf-spaces">Cerebrospinal Fluid (CSF) spaces</h2>

<p>The brain is surrounded by CSF fluids within the sulci, fissures and basal cisterns.</p>

<p>It serves three main functions:</p>

<ul>
  <li>Protection – acts as a cushion for the brain, limiting neural damage in cranial injuries.</li>
  <li>Buoyancy – immersion in CSF reduces net weight of brain to ~25 grams. This prevents excessive pressure on the base of the brain.</li>
  <li>Chemical stability – example. it maintains low extracellular K+ for synaptic transmission.</li>
</ul>

<blockquote>
  <p>CSF has lower density than gray or white matter (discussed ahead), hence appear darker in CT scans.</p>
</blockquote>

<ul>
  <li>Gyrus = a fold of the brain surface (plural = gyri)</li>
  <li>Sulcus = furrow between the gyri which contains CSF (plural = sulci)</li>
</ul>

<figure>
<img src="/assets/img/brain-anatomy-ct/gyrus.png" width="400" />
<figcaption>It suggestes bleeds or swelling in brain.</figcaption>
</figure>

<h3 id="fissures">Fissures</h3>

<p>These are components of brain which serperate the two halves of the brain.</p>

<blockquote>
  <p>Two <strong>hemispheres</strong> are seperated by <strong>interhemispheric fissure</strong>.</p>
</blockquote>

<blockquote>
  <p>Two <strong>lobes</strong> - frontal and temporal lobes are seperated by <strong>Sylvian</strong></p>
</blockquote>

<figure>
<img src="/assets/img/brain-anatomy-ct/fissures.png" width="400" />
<figcaption>Interhemispheric fissure and Sylvian.</figcaption>
</figure>

<h3 id="ventricles">Ventricles</h3>

<p>Ventricles are spaces inside brain containing CSF and responsible for its production. There are 4 ventricles in brain:</p>
<ul>
  <li>Lateral Ventricles (left and right)</li>
  <li>Third Ventricle</li>
  <li>Fourth Ventricle</li>
</ul>

<figure>
<img src="/assets/img/brain-anatomy-ct/ventricles-all.png" width="600" />
<figcaption>Ventricles</figcaption>
</figure>

<p><strong>Lateral ventricles</strong></p>

<p>They are located on either side of brain.</p>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/ventricles_lat.jpg" width="600" />
<figcaption>Lateral Ventricle</figcaption>
</figure>

<p>Function:</p>
<ul>
  <li>It ensures the communication between the third and fourth ventricles.</li>
  <li>It is to house the cerebrospinal fluid (CSF) and provide the passage for its circulation.</li>
</ul>

<p>The marked choroid plexus produces CSF and is usually calcified (hardened by deposition of or conversion into calcium carbonate or another insoluble calcium compound) for adults.</p>

<blockquote>
  <p><strong>The volume of the lateral ventricles increases with age.</strong></p>
</blockquote>

<p><strong>Third Ventricle</strong></p>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/ventricles_third.jpg" width="600" />
<figcaption>Third Ventricle</figcaption>
</figure>

<p>Small white colored <code class="language-plaintext highlighter-rouge">Foramen of Monro</code> is a messenger between lateral and third ventricle.</p>

<p>Abnormalities of the third ventricle are associated with various conditions including hydrocephalus, meningitis, and ventriculitis.</p>

<p><strong>Fourth Ventricle</strong></p>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/ventricles_fourth.jpg" width="600" />
<figcaption>Fourth Ventricle</figcaption>
</figure>

<blockquote>
  <p>CSF in the basal cisterns surrounds the brain stem structures.</p>
</blockquote>

<p>It has 2 function:</p>
<ul>
  <li>Production of cerebrospinal fluid (CSF) by choroid plexus</li>
  <li>Circulation of CSF</li>
</ul>

<h2 id="brain-lobes">Brain Lobes</h2>

<p>Brain has paired lobes.</p>

<figure>
<img src="https://www.mayoclinic.org/-/media/kcms/gbs/patient-consumer/images/2013/11/15/17/44/ds00266_ds00810_im03440_bn7_lobesthu_jpg.png" />
<figcaption>Brain Lobes</figcaption>
</figure>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/lobes_parietal.jpg" />
<figcaption>Brain Lobes CT Scan</figcaption>
</figure>

<p><strong>Front Lobe</strong> controls cognitive functions and voluntary movement.</p>

<p><strong>Parietal Lobe</strong> infers about temperature, taste, touch and movement from impulse recieved from receptors.</p>

<p><strong>Occipital Lobe</strong> is primarily responsible for vision.</p>

<p><strong>Temporal Lobe</strong> processes memories, integrating them with sensations of taste, sound, sight and touch.</p>

<blockquote>
  <p>Important point to note is that these lobes aren’t well defined and are difficult to point to hence these are also refered to as ‘regions’. Like temporal region, etc.</p>
</blockquote>

<h2 id="grey-matter-v-white-matter">Grey matter v white matter</h2>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/grey_v_white.jpg" />
<figcaption>Gray Matter vs White Matter</figcaption>
</figure>

<blockquote>
  <p>White matter is located centrally and appears blacker than grey matter due to its relatively low density but pathological processes may increase or decrease the differentiation in density between grey and white matter.</p>
</blockquote>

<p><strong>Gray Matter</strong> comprises <code class="language-plaintext highlighter-rouge">bitcoin (BITC)</code>. Basal ganglia, Insula, Thalamus, and Cortex.</p>

<blockquote>
  <p>Grey matter contains relatively few axons and a higher number of cell bodies.</p>
</blockquote>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/grey.jpg" />
<figcaption>Gray Matter</figcaption>
</figure>

<p>Lets start with cortex, probably most recognized among all. It has huge number of variants in computer science field also :laughing: like <a href="https://engineering.purdue.edu/elab/CortexNet/">cortexnet</a>, <a href="https://jspuij.github.io/Cortex.Net.Docs">Cortex.Net</a>, etc.</p>

<p><strong>Cerebral cortex</strong> is a layer of grey matter formed in gyri over the entire brain surface.</p>

<p>Cerebral cortex is required for voluntary activities, language, speech, and multiple brain functions, such as thinking and memory. In short you conscious behaviour (as per science).</p>

<blockquote>
  <p>The cerebrum is the largest brain structure and part of the forebrain (or prosencephalon). Its prominent outer portion, the cerebral cortex, not only processes sensory and motor information but enables consciousness, our ability to consider ourselves and the outside world.</p>
</blockquote>

<p><a href="https://www.visiblebody.com/learn/nervous/brain#:~:text=The%20cerebrum%20is%20the%20largest,ourselves%20and%20the%20outside%20world.">Source</a></p>

<p>Next big thing in <code class="language-plaintext highlighter-rouge">BITC</code> is probably <strong>thalamus</strong>. It has many essential roles in human physiology.</p>

<blockquote>
  <p><strong>Thalamus</strong> is basically a translator used to convert high level impulses from receptors to low level signals for cerebral cortex.</p>
</blockquote>

<p>As for clinical significance, insults to thalamus may result in thalamic pain syndrome.</p>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/basal_ganglia_2.jpg" />
<figcaption>Thalamus and Basal ganglia</figcaption>
</figure>

<p><strong>Basal ganglia = lentiform nucleus + caudate nucleus</strong></p>

<blockquote>
  <p>Important thing to note about <strong>basal ganglia</strong>, insults to this component may cause movement disorders.</p>
</blockquote>

<p><a href="https://www.flintrehab.com/basal-ganglia-brain-damage/">Symptoms to detect damage basal ganglia</a></p>

<p>The movement disorders are generally well treated using neuroplasticity of brain - ability of brain to repair itself- meaning you excersice the affected part often.</p>

<p>Last of <code class="language-plaintext highlighter-rouge">BITC</code> - Insula. Name may be small but it regulates immune system itself among other major functions such as taste.</p>

<p><strong>White Matter</strong></p>

<p>The internal capsules and corpus callosum are clinically important white matter tracts.</p>

<blockquote>
  <p>White matter has a high content of myelinated axons.</p>
</blockquote>

<p><strong>Corpus Callosum</strong> is visible in Sagittal CT’s and connects white matter of the left and right cerebral hemispheres. Its clinical significance is that it can let lesions of brain to grow from one hemisphere to other. Elsewhere flax acts as a barrier to such invasions.</p>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/corpus_callosum.jpg" />
<figcaption>Corpus Callosum</figcaption>
</figure>

<p><img src="" alt="" /></p>

<p><strong>Internal Capsule</strong></p>

<blockquote>
  <p>Even a small insult to the internal capsule can have a profound affect on motor and sensory function.</p>
</blockquote>

<figure>
<img src="https://www.radiologymasterclass.co.uk/images/ct/ct-brain/anatomy/internal_capsule.jpg" />
<figcaption>Internal Capsule</figcaption>
</figure>

<h2 id="calcified-structures">Calcified Structures</h2>

<p>There are some structures in brain which are normal if calcified.
<a href="https://www.radiologymasterclass.co.uk/tutorials/ct/ct_brain_anatomy/ct_brain_anatomy_choroid_calcified">Please refer this</a>.</p>

<h1 id="final-words">Final Words</h1>

<p>You may also checkout <a href="https://www.radiologycafe.com/medical-students/radiology-basics/head-anatomy">Head and spine anatomy</a>. This provides brain anatomy on slice by slice basis.</p>

<h1 id="references">References</h1>

<ol>
  <li><a href="https://www.radiologymasterclass.co.uk/">radiologymasterclass.co.uk</a></li>
  <li><a href="https://www.kenhub.com/">kenhub.com</a></li>
  <li><a href="https://www.mayoclinic.org/">mayoclinic.org</a></li>
  <li><a href="https://teachmeanatomy.info/">teachmeanatomy.info</a></li>
  <li><a href="https://www.radiologycafe.com/medical-students/radiology-basics/head-anatomy">radiologycafe.com</a></li>
</ol>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/20/pydicom-tutorial"><h3 class="contrast">Introduction to Pydicom</h3></a>
          <br /><span class="smaller">December 31, 2020</span>  <br />
          <div><p><a href="https://pydicom.github.io/">PyDICOM Website</a></p>

<p>Code can be found @ <a href="https://github.com/Ujjwal-9/medical-training/tree/master/dicom-demo">Github</a>. Checkout <code class="language-plaintext highlighter-rouge">dicom-viewer.ipynb</code></p>

<h1 id="abstract">Abstract</h1>

<p>Pydicom is a pure Python package for working with DICOM files. It lets you read, modify and write DICOM data in an easy “pythonic” way.</p>

<h1 id="introduction">Introduction</h1>

<p><strong>Installation</strong></p>

<p>Using pip:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>pip <span class="nb">install </span>pydicom
</code></pre></div></div>

<p>Using conda:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge pydicom
</code></pre></div></div>

<p>Pydicom comes with its own set of dicom images which can be used to go through examples.</p>

<p>They also give <code class="language-plaintext highlighter-rouge">get_dataset.py</code> file to download datasets, which is also included in the <a href="https://github.com/Ujjwal-9/medical-training/tree/master/dicom-demo">github repo</a>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>python get_datasets.py <span class="nt">--show</span>
<span class="nv">$ </span>python get_datasets.py <span class="nt">--output</span> <span class="o">{</span>path<span class="o">}</span>
</code></pre></div></div>

<h1 id="tutorial">Tutorial</h1>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_scan</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="n">slices</span> <span class="o">=</span> <span class="p">[</span><span class="n">pydicom</span><span class="p">.</span><span class="n">dcmread</span><span class="p">(</span><span class="n">path</span> <span class="o">+</span> <span class="s">'/'</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span>               
              <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path</span><span class="p">)]</span>
    <span class="n">slices</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">slices</span> <span class="k">if</span> <span class="s">'SliceLocation'</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
    <span class="n">slices</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">InstanceNumber</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">slice_thickness</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">slices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">ImagePositionPatient</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span>   
                          <span class="n">slices</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">ImagePositionPatient</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">slice_thickness</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">slices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">SliceLocation</span> <span class="o">-</span> 
                          <span class="n">slices</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">SliceLocation</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">slices</span><span class="p">:</span>
        <span class="n">s</span><span class="p">.</span><span class="n">SliceThickness</span> <span class="o">=</span> <span class="n">slice_thickness</span>
    <span class="k">return</span> <span class="n">slices</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">load_scan</code> loads the dicom files and sorts them according to their <code class="language-plaintext highlighter-rouge">Instance Number</code>. It also extracts slice thickness which is a very important parameter in these scans as it is indicative of resolution of the scans. Lower the slice thickness, better the resolution.</p>

<p><strong>Slice Thickness</strong></p>

<blockquote>
  <p>Slice thickness directly impacts the precision of target localization during treatment.</p>
</blockquote>

<figure>
<img src="https://www.materialise.com/sites/default/files/image-uploads/pages/academy/Medical/slice_increment.jpg" />
<figcaption>Slice Increment and Slice Thickness</figcaption>
</figure>

<p>Slice Increment/Spacing refers to the movement of the table/scanner for scanning the next slice.</p>

<blockquote>
  <p>If slice thickness is greater than slice increment than there is anatomical information loss.</p>
</blockquote>

<p>If there is overlap between 2 adjacent slices that is <code class="language-plaintext highlighter-rouge">slice thickness &gt; slice increment</code> than such cases acts as error correction.</p>

<p><strong>HU Scaling</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pixels_hu</span><span class="p">(</span><span class="n">scans</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">s</span><span class="p">.</span><span class="n">pixel_array</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">scans</span><span class="p">])</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>
    
    <span class="c1"># Convert to Hounsfield units (HU)
</span>    <span class="n">slope</span> <span class="o">=</span> <span class="n">scans</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">RescaleSlope</span>
    
    <span class="k">if</span> <span class="n">slope</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">image</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>
        
    <span class="n">image</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">(</span><span class="n">scans</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">RescaleIntercept</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>
</code></pre></div></div>

<p>HU scaling is explained in my <a href="http://ujjwal9.ml/blog/medicine/2020/12/28/dicom-intro.html">dicom standard blog</a>.</p>

<p><strong>Multiplanar reconstruction</strong></p>
<figure><figcaption>Multiplanar view of brain.</figcaption><img src="https://images.ctfassets.net/cnu0m8re1exe/1k0YS9HKpsyurGlnI1Zlky/223cbf9b658b7068925ba7f944a9bb39/sagittal.jpg" /></figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">slices</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">patient_dicom</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="p">.</span><span class="n">SliceLocation</span><span class="p">)</span>

<span class="c1"># pixel aspects, assuming all slices are the same
</span><span class="n">pixel_spacing</span> <span class="o">=</span> <span class="n">slices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">PixelSpacing</span>
<span class="n">slice_thickness</span> <span class="o">=</span> <span class="n">slices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">SliceThickness</span>
<span class="n">ax_aspect</span> <span class="o">=</span> <span class="n">pixel_spacing</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">pixel_spacing</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sag_aspect</span> <span class="o">=</span> <span class="n">pixel_spacing</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">slice_thickness</span>
<span class="n">cor_aspect</span> <span class="o">=</span> <span class="n">slice_thickness</span><span class="o">/</span><span class="n">pixel_spacing</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># create 3D array
</span><span class="n">img_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">slices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">pixel_array</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">img_shape</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slices</span><span class="p">))</span>
<span class="n">img3d</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">img_shape</span><span class="p">)</span>

<span class="c1"># fill 3D array with the images from the files
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">slices</span><span class="p">):</span>
    <span class="n">img2d</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">pixel_array</span>
    <span class="n">img3d</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">img2d</span>
    
<span class="c1"># print(img3d.shape)
</span>
<span class="c1"># plot 3 orthogonal slices
</span><span class="n">a1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">a1</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img3d</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">img_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">a1</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="n">ax_aspect</span><span class="p">)</span>

<span class="n">a2</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">a2</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img3d</span><span class="p">[:,</span> <span class="n">img_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="p">:],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">a2</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="n">sag_aspect</span><span class="p">)</span>

<span class="n">a3</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">a3</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img3d</span><span class="p">[</span><span class="n">img_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:].</span><span class="n">T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">a3</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="n">cor_aspect</span><span class="p">)</span>
</code></pre></div></div>
<figure>
<img src="/assets/img/pydicom-tutorial/mpr.png" />
<figcaption>Multiplanar reformation - axial, sagittal and coronal view</figcaption>
</figure>

<div class="epigraph"><blockquote><p>Multiplanar reformation or reconstruction (MPR) involves the process of converting data from an imaging modality acquired in a certain plane, usually axial, into another plane. It is most commonly performed with thin-slice data from volumetric CT in the axial plane, but it may be accomplished with scanning in any plane and whichever modality capable of cross-sectional imaging, including magnetic resonance imaging (MRI), PET and SPECT.</p><footer>Source, <cite>https://radiopaedia.org/articles/multiplanar-reformation-mpr?lang=us</cite></footer></blockquote></div>

<p><strong>Windowing</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">level</span> <span class="o">=</span> <span class="n">dicom_file</span><span class="p">.</span><span class="n">WindowCenter</span>
<span class="n">width</span> <span class="o">=</span> <span class="n">dicom_file</span><span class="p">.</span><span class="n">WindowWidth</span>
<span class="c1"># ...or set window/level manually to values you want
</span><span class="n">vmin</span> <span class="o">=</span> <span class="n">level</span> <span class="o">-</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span>
<span class="n">vmax</span> <span class="o">=</span> <span class="n">level</span> <span class="o">+</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">hu_pixels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<figure><figcaption>Without Windowing.</figcaption><img src="/assets/img/pydicom-tutorial/windowing-org.png" /></figure>
<figure><figcaption>With Windowing.</figcaption><img src="/assets/img/pydicom-tutorial/windowing-new.png" /></figure>
<figure><figcaption>Windows for various scans.</figcaption><img src="https://www.stepwards.com/wp-content/uploads/2019/12/Screen-Shot-2019-10-06-at-8.45.17-PM-e1577078248771.jpg" /></figure>

<blockquote>
  <p>Brain windows are useful for evaluation of brain hemorrhage, fluid-filled structures including blood vessels and ventricles, and air-filled spaces.</p>
</blockquote>

<blockquote>
  <p>Bone windows are useful for evaluation in the setting of trauma.</p>
</blockquote>

<h1 id="references">References</h1>

<p><a href="https://doi.org/10.1118/1.598500">“Martin J. Murphy. The importance of computed tomography slice thickness in radiographic patient positioning for radiosurgery. “</a></p>

<p><a href="https://www.materialise.com/en/faq/what-difference-between-slice-thickness-and-slice-increment#:~:text=Slice%20thickness%20and%20slice%20increment%20are%20central%20concepts%20that%20surround,4%20mm%20in%20the%20illustration">“What is the difference between slice thickness and slice increment?”</a></p>

<p><a href="https://www.sciencedirect.com/topics/medicine-and-dentistry/hounsfield-scale">“Hounsfield Scale”</a></p>

<p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4220362/">“Usefulness of hounsfield unit and density in the assessment and treatment of urinary stones”</a></p>

<p><a href="https://radiopaedia.org/articles/multiplanar-reformation-mpr?lang=us">“Multiplanar reformation (MPR)”. Dr Daniel J Bell and Dr Francis Fortin et al. radiopaedia.org</a></p>

<blockquote>
  <p><strong>Follow me on twitter <a href="https://twitter.com/theujjwal9">@theujjwal9</a></strong></p>
</blockquote>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/20/dicom-intro"><h3 class="contrast">DICOM Standards</h3></a>
          <br /><span class="smaller">December 28, 2020</span>  <br />
          <div><p><a href="https://www.dicomstandard.org/">DICOM Website!</a></p>

<p><strong>DICOM = Digital Imaging and Communications in Medicine</strong></p>

<p>The DICOM standard includes a file format definition and a network communications protocol that uses TCP/IP to communicate between systems.</p>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 110}
});
</script>

<h1 id="abstract">Abstract</h1>

<p>The DICOM standard was conceptualized and implemented when CT (computed tomography) scans were developed. It takes special care not to lose information when translating medical images and other data to digital format. This is done to keep it as close to the original as possible.</p>

<blockquote>
  <p>The main objective of this new standard was to create an open platform for the communication of medical images and related data.</p>
</blockquote>

<h1 id="introduction">Introduction</h1>

<p>The contents of the DICOM standard go far beyond a definition of an exchange format for medical image data.</p>

<p>DICOM defines:</p>

<ol>
  <li>Data structures (formats) for medical images and related data</li>
  <li>Network-oriented services, e.g.:
    <ul>
      <li>Image transmission</li>
      <li>Query of an image archive (PACS)</li>
      <li>Print (hardcopy)</li>
      <li>RIS - PACS - modality integration</li>
    </ul>
  </li>
  <li>Formats for storage media exchange</li>
  <li>Requirements for conforming devices and programs</li>
</ol>

<p><img src="/assets/img/dicom-intro/dicom-workflow.png" width="600" /> <label for="sidenote-id" class="margin-toggle"> ⊕</label><input type="checkbox" id="sidenote-id" class="margin-toggle" /><span class="marginnote"><a href="https://www.softneta.com/solutions/dicom-anonymization-for-medical-education-and-research/">[Image Source]</a> </span></p>

<blockquote>
  <p>PACS (Picture Archiving and Communication System) provides economical storage of, and convenient access to, images from multiple modalities (source machine types).</p>
</blockquote>

<blockquote>
  <p>RIS (Radiology Information System) is a computerized database used by radiology departments to store, manipulate, and distribute patient radiological data and imagery.</p>
</blockquote>

<p><img src="/assets/img/dicom-intro/dicom-protocol-workflow.png" width="600" /> <label for="sidenote-id" class="margin-toggle"> ⊕</label><input type="checkbox" id="sidenote-id" class="margin-toggle" /><span class="marginnote"><a href="https://www.extrahop.com/company/blog/2016/introduction-to-dicom-protocol/">[Image Source]</a> </span></p>

<p>PACS-RIS integration improves the flow of images for radiologists. They communicate using a set of commands (called HL7) concerned with Admission/Discharge/Transfer (ADT) and Order/Entry.</p>

<h2 id="dicom-data-structures">DICOM Data Structures</h2>

<p>DICOM consists of a list of image attributes which contain vast amounts of image and medical information:</p>
<ul>
  <li>Patient information (name, sex, identification number)</li>
  <li>Modality and imaging procedure information (device parameters, calibration, radiation dose, contrast media)</li>
  <li>Image information (resolution, windowing)</li>
</ul>

<p>DICOM goes to great lengths and defines the significance of each data element in a multitude of cases. It defines if an attribute is required, optional, or important for certain cases. But this feature comes at a cost:</p>
<ol>
  <li>Image objects are frequently incomplete: There is inconsistency in filling all the fields with data. Some fields in image objects are often left blank and some are filled with incorrect data.</li>
  <li>Another problem occurs when displaying an image on a device made by a different manufacturer, because different imaging equipment uses different amplitude ranges and the same number of allocated bits. In that case, images can be displayed as underexposed or overexposed with poor contrast, so those parameters should be adjusted manually.</li>
</ol>

<h2 id="dicom-network-services">DICOM Network Services</h2>

<p>This service is based on the client-server concept. DICOM applications establish connections to exchange information. In addition to image transmission, there are other features:</p>

<ul>
  <li>Image Archive Service: Search images in a PACS archive by certain criteria (patient, time of creation of the images, modality etc.) and selectively download images from this archive</li>
  <li>Print Service: Gives access to cameras and printers over a network</li>
  <li>Modality Worklist Service: Download updated information regarding patients using the above-described PACS-RIS system</li>
</ul>

<h3 id="patient-model">Patient Model</h3>

<p>Queries to image archives (PACS) are made in 4 levels of DICOM hierarchy:</p>
<figure>
<svg width="300" height="300" viewBox="0 0 596 1628" fill="none" xmlns="http://www.w3.org/2000/svg">
<g filter="url(#filter0_d)">
<rect x="4" width="588" height="252" fill="white" />
<rect x="4.5" y="0.5" width="587" height="251" stroke="black" />
</g>
<path d="M123.641 92.8C129.465 92.8 134.521 93.76 138.809 95.68C143.161 97.6 146.489 100.352 148.793 103.936C151.097 107.52 152.249 111.776 152.249 116.704C152.249 121.568 151.097 125.824 148.793 129.472C146.489 133.056 143.161 135.808 138.809 137.728C134.521 139.648 129.465 140.608 123.641 140.608H108.473V160H95.9928V92.8H123.641ZM123.065 130.048C128.505 130.048 132.633 128.896 135.449 126.592C138.265 124.288 139.673 120.992 139.673 116.704C139.673 112.416 138.265 109.12 135.449 106.816C132.633 104.512 128.505 103.36 123.065 103.36H108.473V130.048H123.065ZM204.921 144.448H171.321L164.697 160H151.832L182.073 92.8H194.361L224.697 160H211.641L204.921 144.448ZM200.792 134.656L188.121 105.28L175.545 134.656H200.792ZM243.5 103.36H221.228V92.8H278.252V103.36H255.98V160H243.5V103.36ZM287.618 92.8H300.098V160H287.618V92.8ZM368.674 149.536V160H318.274V92.8H367.33V103.264H330.754V120.736H363.202V131.008H330.754V149.536H368.674ZM442.299 92.8V160H432.027L394.971 114.496V160H382.587V92.8H392.859L429.915 138.304V92.8H442.299ZM473.937 103.36H451.665V92.8H508.689V103.36H486.417V160H473.937V103.36Z" fill="black" />
<g filter="url(#filter1_d)">
<rect x="4" y="456" width="588" height="252" fill="white" />
<rect x="4.5" y="456.5" width="587" height="251" stroke="black" />
</g>
<path d="M162.059 616.96C156.875 616.96 151.851 616.224 146.987 614.752C142.187 613.28 138.379 611.328 135.563 608.896L139.883 599.2C142.635 601.376 145.995 603.168 149.963 604.576C153.995 605.92 158.027 606.592 162.059 606.592C167.051 606.592 170.763 605.792 173.195 604.192C175.691 602.592 176.939 600.48 176.939 597.856C176.939 595.936 176.235 594.368 174.827 593.152C173.483 591.872 171.755 590.88 169.643 590.176C167.531 589.472 164.651 588.672 161.003 587.776C155.883 586.56 151.723 585.344 148.523 584.128C145.387 582.912 142.667 581.024 140.363 578.464C138.123 575.84 137.003 572.32 137.003 567.904C137.003 564.192 137.995 560.832 139.979 557.824C142.027 554.752 145.067 552.32 149.099 550.528C153.195 548.736 158.187 547.84 164.075 547.84C168.171 547.84 172.203 548.352 176.171 549.376C180.139 550.4 183.563 551.872 186.443 553.792L182.507 563.488C179.563 561.76 176.491 560.448 173.291 559.552C170.091 558.656 166.987 558.208 163.979 558.208C159.051 558.208 155.371 559.04 152.939 560.704C150.571 562.368 149.387 564.576 149.387 567.328C149.387 569.248 150.059 570.816 151.403 572.032C152.811 573.248 154.571 574.208 156.683 574.912C158.795 575.616 161.675 576.416 165.323 577.312C170.315 578.464 174.411 579.68 177.611 580.96C180.811 582.176 183.531 584.064 185.771 586.624C188.075 589.184 189.227 592.64 189.227 596.992C189.227 600.704 188.203 604.064 186.155 607.072C184.171 610.08 181.131 612.48 177.035 614.272C172.939 616.064 167.947 616.96 162.059 616.96ZM214.297 559.36H192.025V548.8H249.049V559.36H226.777V616H214.297V559.36ZM286.092 616.96C276.812 616.96 269.58 614.368 264.396 609.184C259.212 603.936 256.62 596.448 256.62 586.72V548.8H269.1V586.24C269.1 599.424 274.796 606.016 286.188 606.016C297.516 606.016 303.18 599.424 303.18 586.24V548.8H315.468V586.72C315.468 596.448 312.876 603.936 307.692 609.184C302.572 614.368 295.372 616.96 286.092 616.96ZM332.946 548.8H362.322C369.49 548.8 375.858 550.208 381.426 553.024C386.994 555.776 391.314 559.712 394.386 564.832C397.458 569.888 398.994 575.744 398.994 582.4C398.994 589.056 397.458 594.944 394.386 600.064C391.314 605.12 386.994 609.056 381.426 611.872C375.858 614.624 369.49 616 362.322 616H332.946V548.8ZM361.746 605.44C366.674 605.44 370.994 604.512 374.706 602.656C378.482 600.736 381.362 598.048 383.346 594.592C385.394 591.072 386.418 587.008 386.418 582.4C386.418 577.792 385.394 573.76 383.346 570.304C381.362 566.784 378.482 564.096 374.706 562.24C370.994 560.32 366.674 559.36 361.746 559.36H345.426V605.44H361.746ZM438.344 592.288V616H425.864V592.48L399.464 548.8H412.808L432.488 581.536L452.36 548.8H464.648L438.344 592.288Z" fill="black" />
<g filter="url(#filter2_d)">
<rect x="4" y="912" width="588" height="252" fill="white" />
<rect x="4.5" y="912.5" width="587" height="251" stroke="black" />
</g>
<path d="M152.731 1072.96C147.547 1072.96 142.523 1072.22 137.659 1070.75C132.859 1069.28 129.051 1067.33 126.235 1064.9L130.555 1055.2C133.307 1057.38 136.667 1059.17 140.635 1060.58C144.667 1061.92 148.699 1062.59 152.731 1062.59C157.723 1062.59 161.435 1061.79 163.867 1060.19C166.363 1058.59 167.611 1056.48 167.611 1053.86C167.611 1051.94 166.907 1050.37 165.499 1049.15C164.155 1047.87 162.427 1046.88 160.315 1046.18C158.203 1045.47 155.323 1044.67 151.675 1043.78C146.555 1042.56 142.395 1041.34 139.195 1040.13C136.059 1038.91 133.339 1037.02 131.035 1034.46C128.795 1031.84 127.675 1028.32 127.675 1023.9C127.675 1020.19 128.667 1016.83 130.651 1013.82C132.699 1010.75 135.739 1008.32 139.771 1006.53C143.867 1004.74 148.859 1003.84 154.747 1003.84C158.843 1003.84 162.875 1004.35 166.843 1005.38C170.811 1006.4 174.235 1007.87 177.115 1009.79L173.179 1019.49C170.235 1017.76 167.163 1016.45 163.963 1015.55C160.763 1014.66 157.659 1014.21 154.651 1014.21C149.723 1014.21 146.043 1015.04 143.611 1016.7C141.243 1018.37 140.059 1020.58 140.059 1023.33C140.059 1025.25 140.731 1026.82 142.075 1028.03C143.483 1029.25 145.243 1030.21 147.355 1030.91C149.467 1031.62 152.347 1032.42 155.995 1033.31C160.987 1034.46 165.083 1035.68 168.283 1036.96C171.483 1038.18 174.203 1040.06 176.443 1042.62C178.747 1045.18 179.899 1048.64 179.899 1052.99C179.899 1056.7 178.875 1060.06 176.827 1063.07C174.843 1066.08 171.803 1068.48 167.707 1070.27C163.611 1072.06 158.619 1072.96 152.731 1072.96ZM242.674 1061.54V1072H192.274V1004.8H241.33V1015.26H204.754V1032.74H237.202V1043.01H204.754V1061.54H242.674ZM300.555 1072L286.827 1052.32C286.251 1052.38 285.387 1052.42 284.235 1052.42H269.067V1072H256.587V1004.8H284.235C290.059 1004.8 295.115 1005.76 299.403 1007.68C303.755 1009.6 307.083 1012.35 309.387 1015.94C311.691 1019.52 312.843 1023.78 312.843 1028.7C312.843 1033.76 311.595 1038.11 309.099 1041.76C306.667 1045.41 303.147 1048.13 298.539 1049.92L313.995 1072H300.555ZM300.267 1028.7C300.267 1024.42 298.859 1021.12 296.042 1018.82C293.227 1016.51 289.099 1015.36 283.659 1015.36H269.067V1042.14H283.659C289.099 1042.14 293.227 1040.99 296.042 1038.69C298.859 1036.32 300.267 1032.99 300.267 1028.7ZM326.805 1004.8H339.285V1072H326.805V1004.8ZM407.862 1061.54V1072H357.462V1004.8H406.518V1015.26H369.942V1032.74H402.39V1043.01H369.942V1061.54H407.862ZM442.606 1072.96C437.422 1072.96 432.398 1072.22 427.534 1070.75C422.734 1069.28 418.926 1067.33 416.11 1064.9L420.43 1055.2C423.182 1057.38 426.542 1059.17 430.51 1060.58C434.542 1061.92 438.574 1062.59 442.606 1062.59C447.598 1062.59 451.31 1061.79 453.742 1060.19C456.238 1058.59 457.486 1056.48 457.486 1053.86C457.486 1051.94 456.782 1050.37 455.374 1049.15C454.03 1047.87 452.302 1046.88 450.19 1046.18C448.078 1045.47 445.198 1044.67 441.55 1043.78C436.43 1042.56 432.27 1041.34 429.07 1040.13C425.934 1038.91 423.214 1037.02 420.91 1034.46C418.67 1031.84 417.55 1028.32 417.55 1023.9C417.55 1020.19 418.542 1016.83 420.526 1013.82C422.574 1010.75 425.614 1008.32 429.646 1006.53C433.742 1004.74 438.734 1003.84 444.622 1003.84C448.718 1003.84 452.75 1004.35 456.718 1005.38C460.686 1006.4 464.11 1007.87 466.99 1009.79L463.054 1019.49C460.11 1017.76 457.038 1016.45 453.838 1015.55C450.638 1014.66 447.534 1014.21 444.526 1014.21C439.598 1014.21 435.918 1015.04 433.486 1016.7C431.118 1018.37 429.934 1020.58 429.934 1023.33C429.934 1025.25 430.606 1026.82 431.95 1028.03C433.358 1029.25 435.118 1030.21 437.23 1030.91C439.342 1031.62 442.222 1032.42 445.87 1033.31C450.862 1034.46 454.958 1035.68 458.158 1036.96C461.358 1038.18 464.078 1040.06 466.318 1042.62C468.622 1045.18 469.774 1048.64 469.774 1052.99C469.774 1056.7 468.75 1060.06 466.702 1063.07C464.718 1066.08 461.678 1068.48 457.582 1070.27C453.486 1072.06 448.494 1072.96 442.606 1072.96Z" fill="black" />
<g filter="url(#filter3_d)">
<rect x="4" y="1368" width="588" height="252" fill="white" />
<rect x="4.5" y="1368.5" width="587" height="251" stroke="black" />
</g>
<path d="M54.0865 1460.8H66.5665V1528H54.0865V1460.8ZM144.455 1460.8V1528H134.183L97.1268 1482.5V1528H84.7428V1460.8H95.0148L132.071 1506.3V1460.8H144.455ZM183.294 1528.96C178.11 1528.96 173.086 1528.22 168.222 1526.75C163.422 1525.28 159.614 1523.33 156.798 1520.9L161.118 1511.2C163.87 1513.38 167.23 1515.17 171.198 1516.58C175.23 1517.92 179.262 1518.59 183.294 1518.59C188.286 1518.59 191.998 1517.79 194.43 1516.19C196.926 1514.59 198.174 1512.48 198.174 1509.86C198.174 1507.94 197.47 1506.37 196.062 1505.15C194.718 1503.87 192.99 1502.88 190.878 1502.18C188.766 1501.47 185.886 1500.67 182.238 1499.78C177.118 1498.56 172.958 1497.34 169.758 1496.13C166.622 1494.91 163.902 1493.02 161.598 1490.46C159.358 1487.84 158.238 1484.32 158.238 1479.9C158.238 1476.19 159.23 1472.83 161.214 1469.82C163.262 1466.75 166.302 1464.32 170.334 1462.53C174.43 1460.74 179.422 1459.84 185.31 1459.84C189.406 1459.84 193.438 1460.35 197.406 1461.38C201.374 1462.4 204.798 1463.87 207.677 1465.79L203.742 1475.49C200.798 1473.76 197.726 1472.45 194.526 1471.55C191.326 1470.66 188.222 1470.21 185.214 1470.21C180.286 1470.21 176.606 1471.04 174.174 1472.7C171.806 1474.37 170.622 1476.58 170.622 1479.33C170.622 1481.25 171.294 1482.82 172.638 1484.03C174.046 1485.25 175.806 1486.21 177.918 1486.91C180.03 1487.62 182.91 1488.42 186.558 1489.31C191.55 1490.46 195.646 1491.68 198.846 1492.96C202.046 1494.18 204.766 1496.06 207.006 1498.62C209.31 1501.18 210.462 1504.64 210.462 1508.99C210.462 1512.7 209.438 1516.06 207.39 1519.07C205.406 1522.08 202.366 1524.48 198.27 1526.27C194.174 1528.06 189.182 1528.96 183.294 1528.96ZM235.531 1471.36H213.259V1460.8H270.283V1471.36H248.011V1528H235.531V1471.36ZM319.858 1512.45H286.258L279.634 1528H266.77L297.01 1460.8H309.298L339.634 1528H326.578L319.858 1512.45ZM315.73 1502.66L303.058 1473.28L290.482 1502.66H315.73ZM407.892 1460.8V1528H397.62L360.564 1482.5V1528H348.18V1460.8H358.452L395.508 1506.3V1460.8H407.892ZM457.003 1528.96C450.219 1528.96 444.075 1527.49 438.571 1524.54C433.131 1521.54 428.843 1517.41 425.707 1512.16C422.635 1506.91 421.099 1500.99 421.099 1494.4C421.099 1487.81 422.667 1481.89 425.803 1476.64C428.939 1471.39 433.227 1467.3 438.667 1464.35C444.171 1461.34 450.315 1459.84 457.099 1459.84C462.603 1459.84 467.627 1460.8 472.171 1462.72C476.715 1464.64 480.555 1467.42 483.691 1471.07L475.627 1478.66C470.763 1473.41 464.779 1470.78 457.675 1470.78C453.067 1470.78 448.939 1471.81 445.291 1473.86C441.643 1475.84 438.795 1478.62 436.747 1482.21C434.699 1485.79 433.675 1489.86 433.675 1494.4C433.675 1498.94 434.699 1503.01 436.747 1506.59C438.795 1510.18 441.643 1512.99 445.291 1515.04C448.939 1517.02 453.067 1518.02 457.675 1518.02C464.779 1518.02 470.763 1515.36 475.627 1510.05L483.691 1517.73C480.555 1521.38 476.683 1524.16 472.075 1526.08C467.531 1528 462.507 1528.96 457.003 1528.96ZM546.049 1517.54V1528H495.649V1460.8H544.705V1471.26H508.129V1488.74H540.577V1499.01H508.129V1517.54H546.049Z" fill="black" />
<line x1="295.5" y1="456.479" x2="295.5" y2="252" stroke="black" stroke-width="5" />
<line x1="295.5" y1="912.48" x2="295.5" y2="708.001" stroke="black" stroke-width="5" />
<line x1="295.5" y1="1368.48" x2="295.5" y2="1164" stroke="black" stroke-width="5" />
<defs>
<filter id="filter0_d" x="0" y="0" width="596" height="260" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB">
<feFlood flood-opacity="0" result="BackgroundImageFix" />
<feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" />
<feOffset dy="4" />
<feGaussianBlur stdDeviation="2" />
<feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.25 0" />
<feBlend mode="normal" in2="BackgroundImageFix" result="effect1_dropShadow" />
<feBlend mode="normal" in="SourceGraphic" in2="effect1_dropShadow" result="shape" />
</filter>
<filter id="filter1_d" x="0" y="456" width="596" height="260" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB">
<feFlood flood-opacity="0" result="BackgroundImageFix" />
<feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" />
<feOffset dy="4" />
<feGaussianBlur stdDeviation="2" />
<feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.25 0" />
<feBlend mode="normal" in2="BackgroundImageFix" result="effect1_dropShadow" />
<feBlend mode="normal" in="SourceGraphic" in2="effect1_dropShadow" result="shape" />
</filter>
<filter id="filter2_d" x="0" y="912" width="596" height="260" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB">
<feFlood flood-opacity="0" result="BackgroundImageFix" />
<feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" />
<feOffset dy="4" />
<feGaussianBlur stdDeviation="2" />
<feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.25 0" />
<feBlend mode="normal" in2="BackgroundImageFix" result="effect1_dropShadow" />
<feBlend mode="normal" in="SourceGraphic" in2="effect1_dropShadow" result="shape" />
</filter>
<filter id="filter3_d" x="0" y="1368" width="596" height="260" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB">
<feFlood flood-opacity="0" result="BackgroundImageFix" />
<feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" />
<feOffset dy="4" />
<feGaussianBlur stdDeviation="2" />
<feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.25 0" />
<feBlend mode="normal" in2="BackgroundImageFix" result="effect1_dropShadow" />
<feBlend mode="normal" in="SourceGraphic" in2="effect1_dropShadow" result="shape" />
</filter>
</defs>
</svg>
<figcaption>DICOM Data Model</figcaption>
</figure>

<p>Patient has  studies, studies have series which are scans and scans may have multiple instances or images (which are slices in CT scans).</p>

<p><strong>Patient Level</strong></p>
<ul>
  <li>How many studies are there for this patient</li>
  <li>How many Series are there for this patient (in all studies)</li>
  <li>How many Instances (images) are there for this patient (in all series)</li>
</ul>

<p><strong>Study Level</strong> (Patient and Study roots)</p>
<ul>
  <li>How many Series are there in this study</li>
  <li>How many Instances (images) are there in this study (in all series)</li>
</ul>

<p><strong>Series Level</strong></p>
<ul>
  <li>How many Instances (images) are there in this series</li>
</ul>

<p><strong>Instance Level</strong></p>

<p><label for="table-1-id" class="margin-toggle"> ⊕</label><input type="checkbox" id="table-1-id" class="margin-toggle" /><span class="marginnote">Table is sourced from <a href="https://www.medicalconnections.co.uk/kb/counting-studies-series-and-instances/">here</a> </span></p>
<div class="table-wrapper">
<table class="booktabs">
<thead>
<tr>
<th>Attribute Name</th>
<th>Tag</th>
<th>Attribute Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>Number of Patient Related Studies</td>
<td>(0020,1200)</td>
<td>The number of studies that match the Patient level Query/Retrieve search criteria</td>
</tr>

<tr>
<td>Number of Patient Related Series</td>
<td>(0020,1202)</td>
<td>The number of series that match the Patient level Query/Retrieve search criteria</td>
</tr>

<tr>
<td>Number of Patient Related Instances</td>
<td>(0020,1204)</td>
<td>The number of composite object instances that match the Patient level Query/Retrieve search criteria</td>
</tr>

<tr>
<td>Number of Study Related Series</td>
<td>(0020,1206)</td>
<td>The number of series that match the Study level Query/Retrieve search criteria</td>
</tr>

<tr>
<td>Number of Study Related Instances</td>
<td>(0020,1208)</td>
<td>The number of composite object instances that match the Study level Query/Retrieve search criteria</td>
</tr>

<tr>
<td>Number of Series Related Instances</td>
<td>(0020,1209)</td>
<td>The number of composite object instances in a Series that match the Series level Query/Retrieve search criteria</td>
</tr>

<tr>
<td>SOP Classes in Study</td>
<td>(0008,0062)</td>
<td>The SOP Classes contained in the Study.</td>
</tr>
</tbody>
</table>

</div>

<h2 id="media-exchange">Media Exchange</h2>

<p>DICOM defines <strong>application profiles</strong> which defines how media is exchanged:</p>
<ul>
  <li>Encoding formats and compression schemes used (e. g. only uncompressed or loss-less JPEG)</li>
  <li>Storage medium used</li>
  <li>Images from which modalities may be present on the medium (X-Ray Angiography images, etc)</li>
</ul>

<p>DICOM directory: each dicom medium contains this directory which contains information (patient name, modality, unique identifiers etc.) for all images which are captured on the medium.</p>

<h2 id="device-conformance">Device Conformance</h2>

<p>Each DICOM supporting device must also specify which DICOM services and options are supported, which extensions and peculiarities have been implemented by the vendor, and how the device communicates with other DICOM systems.</p>

<h1 id="dicom-file-format">DICOM File Format</h1>
<p>The DICOM standard is divided in 2 parts:</p>

<p>A DICOM file consists of a header and image data sets.</p>

<p><img src="/assets/img/dicom-intro/packet.jpg" width="250" /> <label for="sidenote-id" class="margin-toggle"> ⊕</label><input type="checkbox" id="sidenote-id" class="margin-toggle" /><span class="marginnote">Packet structure of dicom file. </span></p>

<p><strong>Preamble</strong> is used to access the images and other data in DICOM file.</p>

<p><strong>Prefix</strong> contains the string “DICM” as uppercase characters.</p>

<p><strong>Data Set</strong> is the representation of real world information.</p>

<p><strong>Data Elements</strong>. There are 5 types of Data elements:</p>
<ul>
  <li>Type 1 Required Data elements,</li>
  <li>Type 1C Conditional Data Elements,</li>
  <li>Type 2 Required Data Elements,</li>
  <li>Type 2C Conditional Data Elements</li>
  <li>Type 3 optional Data Elements.</li>
</ul>

<h1 id="clinical-terms">Clinical Terms</h1>

<p>Some clinical terms to be aware of!</p>

<h2 id="slice-thickness">Slice Thickness</h2>

<p>Slice thickness refers to the (often axial) resolution of the scan.</p>

<div class="epigraph"><blockquote><p>Patient Position (0018, 5100) will tell you if the patient was scanned head-first supine, feet-first prone, head-first prone, etc. Instance Number (0020, 0013), also commonly known as slice number, contains no information about spatial location and isn't even guaranteed to be unique. Slice Location (0020, 1041) is useful, if it exists, but you can't count on it always existing because it's a Type 3 (optional) attribute. To have a robust solution, you need to use Image Position Patient (0020, 0032) together with Image Orientation Patient (0020, 0037) and Patient Position (0018, 5100) to properly order the slices in space. Image Position Patient gives you a vector from the origin to the center of the first transmitted pixel of the image. Image Orientation Patient gives you vectors for the orientation of the rows and columns of the image in space. Patient Position tells you how the patient was placed on the table relative to the coordinate system.</p><footer>Source, <cite>https://stackoverflow.com/questions/6597843/dicom-slice-ordering</cite></footer></blockquote></div>

<h2 id="hu-scaling-hounsfield-scale">Hu Scaling (Hounsfield scale)</h2>
<p>HU is a quantitative scale for describing radiodensity. HU’s is standardized across all CT scans regardless of the scanner detector.</p>

\[HU = 1000 * \frac{\mu - \mu_{water}}{\mu_{water} - \mu_{air}}\]

<p>Where \(\mu\) is <a href="https://radiopaedia.org/articles/linear-attenuation-coefficient?lang=us">linear attenuation coefficient</a>.</p>

<p><label for="table-1-id" class="margin-toggle"> ⊕</label><input type="checkbox" id="table-1-id" class="margin-toggle" /><span class="marginnote">Here’s a quick list of a few useful ones, sourced from <a href="https://en.wikipedia.org/wiki/Hounsfield_scale">Wikipedia</a>. </span></p>
<div class="table-wrapper">
<table class="booktabs">
<tbody><tr>
<th colspan="2">Substance</th>
<th>HU
</th></tr>
<tr>
<td colspan="2"><a href="/wiki/Air" class="mw-redirect" title="Air">Air</a>
</td>
<td>−1000
</td></tr>
<tr>
<td colspan="2"><a href="/wiki/Fat" title="Fat">Fat</a>
</td>
<td>−120 to −90
</td></tr>
<tr>
<td colspan="2">Soft tissue on <a href="/wiki/Contrast_CT" title="Contrast CT">contrast CT</a>
</td>
<td>+100 to +300
</td></tr>
<tr>
<td rowspan="2"><a href="/wiki/Bone" title="Bone">Bone</a></td>
<td>Cancellous</td>
<td>+300 to +400
</td></tr>
<tr>
<td>Cortical</td>
<td>+1800 to +1900
</td></tr>
</tbody></table>
</div>

<h2 id="windowing">Windowing</h2>
<p>Since it is difficult to recognize 4000 shades of gray easily, we use windowing. It limits the number of Hounsfield units that are displayed.</p>

<p>For example, if we want to examine the soft tissue in one CT scan we can use a window level of 40 and a window Width of 80 this will cover 40 units below and above the window level and the tissues with CT numbers outside this range will appear either black or white. A narrow range provides a higher contrast.</p>

<p>The <strong>window width</strong> is the range of the grayscale that can be displayed. The center of grayscale range is referred to as the <strong>window level</strong>.</p>

<figure>
<img src="/assets/img/dicom-intro/window.png" width="600" />
<figcaption>Window width vs Window level</figcaption>
</figure>

<p>You can go through <a href="https://kevalnagda.github.io/ct-windowing">this blog</a> to learn more about windowing.</p>

<h1 id="walkthrough-ohif-viewer">Walkthrough Ohif Viewer</h1>

<p>This section contains steps on how to use ohif viewer for viewing dicom file based medical records (studies, series, instances).</p>

<h2 id="homepage">Homepage</h2>
<p>Here you see the studies you have uploaded. Each study have series and these series have instances / images.</p>

<p><img src="/assets/img/dicom-intro/ohif-home.png" /></p>

<p>Studies can be uploaded using <code class="language-plaintext highlighter-rouge">+</code> button on right. Example is shown below.</p>

<p><img src="/assets/img/dicom-intro/ohif-home.gif" /></p>

<h2 id="details-on-scan-view">Details on Scan View</h2>

<p><img src="/assets/img/dicom-intro/ct-scan-view.png" /></p>

<h2 id="calibrating-windowing">Calibrating Windowing</h2>

<p><img src="/assets/img/dicom-intro/ohif-change-WL.gif" /></p>

<p>Vertical mouse movement changes window level and horizontal movement changes window width.</p>

<h2 id="visualization-of-slices">Visualization of slices</h2>

<p><img src="/assets/img/dicom-intro/ohif-slice.gif" /></p>

<h2 id="2d-multiplanar-reconstruction-mpr">2D Multiplanar Reconstruction (MPR)</h2>

<p><img src="/assets/img/dicom-intro/ohif-2d-mpr.png" /></p>

<p>The MPR tool provided within the Viewer can be used to reconstruct images in orthogonal planes (coronal, sagittal, axial or oblique, depending on what the base image plane is). This can help to create a visualization of the anatomy which was not possible using base images alone.</p>

<h2 id="report-view">Report view</h2>

<p><img src="/assets/img/dicom-intro/ohif-report.png" /></p>

<p>Report view has report on doc/pdf format displayed in viewer.</p>

<h1 id="references">References</h1>

<p><a href="https://www.digitmedicine.com/article.asp?issn=2226-8561;year=2015;volume=1;issue=2;spage=63;epage=66;aulast=Gupta">“Significance of digital imaging and communication in medicine in digital imaging”. digitmedicine.com</a></p>

<p><a href="http://www.medico-eng.com/en/products.php?id=71">“Picture archiving and communication system”. medico-eng.com</a></p>

<p><a href="https://pubmed.ncbi.nlm.nih.gov/12080929/">“The case for RIS/PACS integration”. PUBMED NCBI</a></p>

<p><a href="https://www.medicalconnections.co.uk/kb/counting-studies-series-and-instances/">“Counting Studies, Series and Instances”. medicalconnections.co.uk</a></p>

<blockquote>
  <p><strong>Follow me on twitter <a href="https://twitter.com/theujjwal9">@theujjwal9</a></strong></p>
</blockquote>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/20/docker"><h3 class="contrast">Docker Overview</h3></a>
          <br /><span class="smaller">December 24, 2020</span>  <br />
          <div><p><a href="https://docker.com/">Docker Website!</a></p>

<p>Check out my project code @ <a href="https://github.com/Ujjwal-9/medical-training/tree/master/dockerized-run">Github</a></p>

<h1 id="abstract">Abstract</h1>
<p>We often try to simplify things but usually end up making them much more difficult. Similar is the case with code. We code, install additional dependencies, and remove redundancies. With this 3-step process, we sometimes end up with a very difficult process to explain how to reproduce the results and rerun the experiments. This blog explains Docker, which is a tool designed to make it easier to create, deploy, and run applications by using containers.</p>

<h1 id="earlier-work">Earlier Work</h1>
<p>Before Docker was introduced, virtualization of resources was used which provided independent virtual machines for clients to work upon. But this came with the price of heavy operating systems which may easily exceed over <em>1GB</em> despite supporting light applications (around 300MB). This drawback led to the advent of containers (Docker).</p>

<figure>
  <img src="/assets/img/docker/docker-vs-vm.png" width="600" style="margin-top: 2rem;" />
  <figcaption style="margin-top: 2rem;">Docker and Virtual Machine Architecture</figcaption>
</figure>

<h1 id="what-is-docker">What is Docker?</h1>
<p>Docker is based on containers which run on shared resources of your PC but in isolation as shown in the following architecture. A container is an efficient mechanism to keep your software components together and maintainable. You can also run multiple containers at the same time to support a service. Docker also provides a mechanism to start all the containers concerned with that service with one command using Docker Compose. We will talk about it later.</p>

<h1 id="dockerfile">Dockerfile</h1>

<p>A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image.</p>

<p>Working Dockerfile for conda environemnt.</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> continuumio/miniconda3</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># Create the environment:</span>
<span class="k">COPY</span><span class="s"> environment.yml .</span>
<span class="k">RUN </span>conda <span class="nb">env </span>create <span class="nt">-f</span> environment.yml

<span class="c"># Make RUN commands use the new environment:</span>
<span class="k">SHELL</span><span class="s"> ["conda", "run", "-n", "myenv", "/bin/bash", "-c"]</span>
<span class="k">RUN </span>python <span class="nt">-c</span> <span class="s2">"import numpy"</span>

<span class="c"># The code to run when container is started:</span>
<span class="k">COPY</span><span class="s"> run.py .</span>
<span class="k">ENTRYPOINT</span><span class="s"> ["conda", "run", "-n", "myenv", "python", "run.py"]</span>
</code></pre></div></div>

<ul>
<li>FROM creates a layer from the continuumio/miniconda3 Docker image.</li>
<li>COPY adds files from your Docker client’s current directory.</li>
<li>RUN builds your application with make.</li>
<li>CMD specifies what command to run within the container.</li>
<li>ENTRYPOINT is to set the image’s main command, allowing that image to be run as though it was that command.</li>
<li>SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is ["/bin/sh", "-c"], and on Windows is ["cmd", "/S", "/C"]</li>
</ul>

<!-- 1. FROM creates a layer from the continuumio/miniconda3 Docker image.

2. COPY adds files from your Docker client’s current directory.

3. RUN builds your application with make.

4. CMD specifies what command to run within the container.

5. ENTRYPOINT is to set the image’s main command, allowing that image to be run as though it was that command.

6. SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is `["/bin/sh", "-c"]`, and on Windows is `["cmd", "/S", "/C"]` -->

<p>Container Image is built using</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker build <span class="nt">-t</span> &lt;app_name&gt;:&lt;label_name&gt; <span class="nb">.</span>
</code></pre></div></div>

<p>To make sure certain package is installed we can add,</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">RUN </span><span class="nb">echo</span> <span class="s2">"Make sure flask is installed:"</span>
<span class="k">RUN </span>python <span class="nt">-c</span> <span class="s2">"import flask"</span>
</code></pre></div></div>

<p>The image defined by your Dockerfile generate containers that have <strong>ephemeral</strong> states. It gets destroyed as soon as process is over. To access files in container we may either bash into container to run the command which generates a new file which we need on our local file system and then use <code class="language-plaintext highlighter-rouge">$docker cp</code> to tranfer to local file system. And if we wish to use files from our local system in docker container we may mount those files either using <code class="language-plaintext highlighter-rouge">COPY</code> when generating container or we may mount it at runtime using volume. We can also specify volume which can be utilized by both local file system and docker.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker build <span class="nt">-t</span> dockerized-run <span class="nb">.</span>
<span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> <span class="nt">-v</span> &lt;PATH-TO_IMAGES&gt;/images:/app/images <span class="nt">--entrypoint</span><span class="o">=</span>/bin/bash dockerized-run

<span class="o">(</span>base<span class="o">)</span> root@b74706db6f68:/app# <span class="nb">ls
</span>environment.yml  images
<span class="o">(</span>base<span class="o">)</span> root@b74706db6f68:/app# <span class="nb">cd </span>images
<span class="o">(</span>base<span class="o">)</span> root@b74706db6f68:/app/images# <span class="nb">ls
</span>out.png  test.png
</code></pre></div></div>

<h2 id="volume">Volume</h2>

<ul>
  <li>Mounting volume at runtime.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> <span class="nt">-v</span> &lt;source-path&gt;:&lt;target-path&gt; &lt;docker-container-name&gt;
</code></pre></div></div>

<p>The above command will run docker with specified volume (-v) mounted in the</p>

<ul>
  <li>Mounting volume at build time using docker compose.</li>
</ul>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">3.9"</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">deeplearning</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span> <span class="s">.</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./images:/app/images</span>
</code></pre></div></div>

<p>Here <code class="language-plaintext highlighter-rouge">volume</code> keyword specifies to mount current directory on local file system to /images on container. So the changes made to those files mounted at /images will also be reflected in local file system.</p>

<h1 id="docker-compose">Docker Compose</h1>

<p>It is used to start multiple containers as a single service. You may start services like react and flask server together as a service.</p>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">3.9"</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">web</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span> <span class="s">.</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">5000:5000"</span>
  <span class="na">redis</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s2">"</span><span class="s">redis:alpine"</span>
</code></pre></div></div>
<p>Taken from docker compose example at <a href="https://docs.docker.com/compose/gettingstarted/">docker compose docs</a>. Here we are starting 2 services 	web and redis. <code class="language-plaintext highlighter-rouge">Web</code> is build using dockerfile as specified by <code class="language-plaintext highlighter-rouge">. (dot)</code> pointing towards dockerfile and <code class="language-plaintext highlighter-rouge">port</code> binds the container and the host machine to the exposed port, 5000. This can also be done using dockerfile by using <code class="language-plaintext highlighter-rouge">EXPOSE</code>.</p>

<p><code class="language-plaintext highlighter-rouge">version</code> is used to specify that we want the details of the version of Docker Compose.</p>

<h1 id="application">Application</h1>
<p>Lets also talk about dockers application in AI research. Now given todays deep learning systems and its other applications, the need for using sameversion of library becomes necessary for inducing reproducibilty in these models. The most common package manager used for python is anaconda.</p>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">name</span><span class="pi">:</span> <span class="s">myenv</span>
<span class="na">channels</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">conda-forge</span>
<span class="na">dependencies</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">python=3.8</span>
  <span class="pi">-</span> <span class="s">numpy</span>
</code></pre></div></div>

<p>We build it with below specified dockerfile.</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> continuumio/miniconda3</span>

<span class="k">COPY</span><span class="s"> environment.yml .</span>
<span class="k">RUN </span>conda <span class="nb">env </span>create <span class="nt">-f</span> environment.yml

<span class="k">ENTRYPOINT</span><span class="s"> ["conda", "run", "-n", "example", \</span>
            "python", "-c", \
            "import numpy; print('success!')"]
</code></pre></div></div>

<p>In this environment, we install Python 3.8 and NumPy, and when we run the image it imports NumPy to make sure everything is working. This can bloat upto <em>950MB</em>. Where is all the disk space being going?</p>

<ol>
  <li>Conda caches downloaded packages.</li>
  <li>Conda base environment where toolchain is installed takes huge space. For example, when we install <code class="language-plaintext highlighter-rouge">continuumio/miniconda3</code>, it comes with its own python which we dont intend use.</li>
</ol>

<p>First problem can be solved by removing those cached files. Second problem is conda specific and hence unavoidable but we can do away with it at runtime.</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The build-stage image:</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">continuumio/miniconda3</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="s">build</span>

<span class="c"># Install the package as normal:</span>
<span class="k">COPY</span><span class="s"> environment.yml .</span>
<span class="k">RUN </span>conda <span class="nb">env </span>create <span class="nt">-f</span> environment.yml

<span class="c"># Install conda-pack:</span>
<span class="k">RUN </span>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge conda-pack

<span class="c"># Use conda-pack to create a standalone enviornment</span>
<span class="c"># in /venv:</span>
<span class="k">RUN </span>conda-pack <span class="nt">-n</span> example <span class="nt">-o</span> /tmp/env.tar <span class="o">&amp;&amp;</span> <span class="se">\
</span>  <span class="nb">mkdir</span> /venv <span class="o">&amp;&amp;</span> <span class="nb">cd</span> /venv <span class="o">&amp;&amp;</span> <span class="nb">tar </span>xf /tmp/env.tar <span class="o">&amp;&amp;</span> <span class="se">\
</span>  <span class="nb">rm</span> /tmp/env.tar

<span class="c"># We've put venv in same path it'll be in final image,</span>
<span class="c"># so now fix up paths:</span>
<span class="k">RUN </span>/venv/bin/conda-unpack


<span class="c"># The runtime-stage image; we can use Debian as the</span>
<span class="c"># base image since the Conda env also includes Python</span>
<span class="c"># for us.</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">debian:buster</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="s">runtime</span>

<span class="c"># Copy /venv from the previous stage:</span>
<span class="k">COPY</span><span class="s"> --from=build /venv /venv</span>

<span class="c"># When image is run, run the code with the environment</span>
<span class="c"># activated:</span>
<span class="k">SHELL</span><span class="s"> ["/bin/bash", "-c"]</span>
<span class="k">ENTRYPOINT</span><span class="s"> source /venv/bin/activate &amp;&amp; \</span>
           python -c "import numpy; print('success!')"
</code></pre></div></div>
<p>The above solutions is provided <a href="https://pythonspeed.com/articles/conda-docker-image-size/">here</a>.</p>

<blockquote>
  <p><strong>Follow me on twitter <a href="https://twitter.com/theujjwal9">@theujjwal9</a></strong></p>
</blockquote>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/19/heartrate-detection"><h3 class="contrast">Heart Rate Detection Using Camera</h3></a>
          <br /><span class="smaller">April 6, 2019</span>  <br />
          <div><p>Euler Video Magnification <a href="https://people.csail.mit.edu/mrub/evm/">Project Link</a> at MIT CSAIL</p>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 100}
});
</script>

<h1 id="introduction">Introduction</h1>

<p>The world generates countless signals as it moves ahead in time, but most of these signals are invisible to the human eye. This is due to the frequency of these signals - either it is too low to be perceived as change or it is too high for us to see the change actually occurring.</p>

<p>Finding a pattern in a seemingly random event is about how we process the given information and convert it into something useful. In this article, we try to connect the dots of predicting heart rate with a camera alone. For this, we tag the video frames with actual heart rate and analyze which features impact accurate prediction. For example, we examine which color signal in our RGB frames contributes towards better prediction and many more such small details.</p>

<p>Take for instance our eyes and our ability to understand facial expressions in others. These changes last long enough that we are able to perceive them. But changes exhibited by heart rate last for a very short period of time and hence become invisible to us. With a camera of sufficiently high frame rate, i.e. almost all modern cameras, these changes become visible. And hence we can find the heart rate.</p>

<p>In this blog, we are going to introduce the algorithms required and demonstrate how to read a person’s heart rate with a camera alone. The technique is called Euler Video Magnification.</p>

<h2 id="measuring-heart-rate">Measuring Heart Rate</h2>

<p>In order to measure your heart rate, doctors have traditionally relied upon technology that is based on monitors with leads that attach to your body. These devices measure one of the following pulses:</p>

<ol>
  <li><strong>Radial Pulse</strong>: Place your pointer and middle fingers on the inside of your opposite wrist just below the thumb and then count how many beats you feel in 1 minute.</li>
  <li><strong>Carotid Pulse</strong>: Place your pointer and middle fingers on the side of your windpipe just below the jawbone and then count how many beats you feel in 1 minute.</li>
  <li><strong>Brachial Pulse</strong>: Another location for checking your pulse is the <a href="https://www.healthline.com/human-body-maps/brachial-artery">brachial artery</a>. This method is used most commonly in young children.</li>
</ol>

<h2 id="overview-of-euler-video-magnification">Overview of Euler Video Magnification</h2>

<p>A computational technique for visualizing subtle color and motion variations in ordinary videos by making the variations larger. It is a <em>microscope for small changes</em> that are hard or impossible for us to see by ourselves. In addition, these small changes can be quantitatively analyzed and used to recover sounds from vibrations in distant objects, characterize material properties, and, in this case, remotely measure a person’s pulse.</p>

<h2 id="concept">Concept</h2>

<p>The general concept behind this algorithm is to first approximate a point on the forehead.</p>

<p>The intensity of this point is then decomposed into different color spaces namely Red, Blue, Green. But we prefer Red and Green colors only as Blue tends to introduce noise in heart rate detection.</p>

<figure><figcaption>Figure 1: Average Color Intensity Variation of a Location specified on the forehead.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*rBux120Fg5f-jZUyB8P-0A.png" /></figure>

<figure><figcaption>Figure 2: Heart Rate extracted by Fourier Transform.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*YCLMq00v1riy1at2b5-kwg.png" /></figure>

<p>The variation in Red and Green color spaces on the location approximated on the forehead is then fed to <strong>Fourier Transform</strong> to convert the function of spatial location on the video frame and time to frequency domain which therefore helps in extracting heart rate.</p>

<h2 id="what-is-fourier-transform">What is Fourier Transform?</h2>

<p>Fourier transform decomposes (also called <em>analysis</em>) a <a href="https://en.wikipedia.org/wiki/Function_(mathematics)">function</a> of time (a <em>signal</em>) into its constituent frequencies. The Fourier transform of a function of time is itself a <a href="https://en.wikipedia.org/wiki/Complex_number">complex</a>-valued function of frequency, whose magnitude component represents the amount of that frequency present in the original function, and whose <a href="https://en.wikipedia.org/wiki/Complex_argument">complex argument</a> is the <a href="https://en.wikipedia.org/wiki/Phase_offset">phase offset</a> of the basic sinusoid in that frequency. You can learn the basics of Fourier transform from this video.</p>

<p><label for="side-note-video" class="margin-toggle"> ⊕</label><input type="checkbox" id="side-note-video" class="margin-toggle" /><span class="marginnote">What is the Fourier Transform? A visual introduction. </span></p>
<style>
  .embed-container {
    position: relative;
    padding-bottom:56.25%;
    padding-top:30px;
    overflow: hidden;
    max-width: 70%;
    margin-bottom: -23%;
  }
  .embed-container iframe,
  .embed-container object,
  .embed-container embed {
    position: absolute;
    top: 0;
    left: 0;
    width: 80%;
    height: 55%;
  }
</style>

<div class="embed-container">
  <iframe title="YouTube video player" width="480" height="360" src="https://www.youtube.com/embed/spUNpyF58BY" frameborder="0" allowfullscreen=""></iframe>
</div>

<p>Now let’s talk about some amplification techniques namely Lagrangian and Eulerian. These techniques will help us in amplifying the particular frequency so that we can see change happening at that rate, in our case heart rate.</p>

<h2 id="lagrangian-perspective">Lagrangian Perspective</h2>

<p>The Lagrangian version of amplification is to analyze the angle of motion of the pixels of interest in the tracking image. For example, if we want to study the flow rate of the river, we take a boat, go down the river, and record the movement of the ship.</p>

<figure><figcaption>Lagrangian perspective of flow.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*eODd2ohQgUxhCOOBvdqwfA.png" /></figure>

<p><strong>However, the Lagrangian perspective approach has the following shortcomings:</strong></p>

<p>➔ It is necessary to accurately track and estimate the trajectory of particles, which requires more computational resources.</p>

<p>➔ The tracking of the particles is performed independently, and with the consideration that the system is closed i.e. there is no transfer of energy in and out of the frame being studied. So the lack of consideration of the overall image is prone to the fact that the image is not closed, thereby affecting the effect of the amplification.</p>

<p>➔ The amplification of the action of the target object is to modify the motion trajectory of the particle. Since the position of the particle changes, it is necessary to fill the original position of the particle, as there is a continuous flow in action and the system is closed therefore some other particle will take the position. This increases the complexity of the algorithm.</p>

<blockquote>
  <p>What is “change” — the trajectory of the pixel of interest over time, such pixels often need to be assisted by manual or other prior knowledge;</p>
</blockquote>

<blockquote>
  <p>Amplify “change” — increase the amplitude of these pixels.</p>
</blockquote>

<h2 id="eulerian-perspective">Eulerian Perspective</h2>

<p>Unlike the Lagrangian perspective, the Eulerian version of amplification does not explicitly track and estimate the motion of the particle but instead fixes the perspective in one place, such as the entire image.</p>

<p>After that, it is assumed that the <strong>entire image is changing,</strong> but the characteristics of the signals like frequency, amplitude, etc. are varying. So we are interested in the change in the signals. In this way, the amplification of the “change” becomes the precipitation and enhancement of the frequency band of interest. For example, to study the flow rate of river water, we can also sit on the shore and observe the change of the river when it passes through a fixed place. This change may contain many components that are not related to the water flow itself, such as leaves falling on the water surface. However, we only focus on the part that best reflects the water flow rate.</p>

<figure><figcaption>Eulerian perspective of flow.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*P__1_fSDtjWqBWnvDNo3Ew.png" /></figure>

<blockquote>
  <p>What is “change” — the whole scene is changing, and the change signals we are interested in are hidden in it;</p>
</blockquote>

<blockquote>
  <p>Amplify “change” — Separate and enhance the signal of interest by means of signal processing.</p>
</blockquote>

<h2 id="explanation">Explanation</h2>

<p>Now, why are we able to extract heart rate from the sequence of frames? It is because the heart pushes blood to every part of the body and to the head particularly (towards the brain), so it changes the color and opacity of the skin. These changes can be detected by analyzing the average red or green component of the frames taken from the camera. We learned the above concepts to be able to understand the different filters required to develop the said application. The analysis is done using the following approach:</p>

<ol>
  <li><strong>Spatial filtering.</strong> Pyramid multiresolution decomposition of the video sequence; This is done to extract features/structures of interest, and to attenuate noise.</li>
  <li><strong>Time domain filtering.</strong> Performing time-domain bandpass filtering on the images of each scale to obtain several frequency bands of interest; This is done using Fourier transform.</li>
  <li><strong>Amplify the filtering result.</strong> The signal of each frequency band is differentially approximated by Taylor series, and the result of linear amplification is approximated; This is why we studied Euler amplification above.</li>
  <li><strong>Composite image.</strong> The amplified image is synthesized.</li>
</ol>

<p>The spatial and temporal processing is used to emphasize subtle temporal changes in a video.</p>

<figure><figcaption>Figure 3: Architecture for Euler Video Magnification.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*PIbHq6tg6djUwnmelOhaDw.png" /></figure>

<p>The video sequence is decomposed into different spatial frequency bands. These bands might be magnified differently due to the difference in their SNR (Signal To Noise Ratio). The goal of spatial processing is simply to increase the temporal signal-to-noise ratio by pooling multiple pixels. Then for the purpose of <strong>computational efficiency and spatial filtering</strong>, the low-pass filter is applied to the frames of the video spatially and then downsampled using <strong>Laplacian Pyramid</strong>.</p>

<h3 id="what-is-the-laplacian-pyramid">What is the Laplacian Pyramid?</h3>

<p>To understand this first of all we need to understand Gaussian Pyramid.</p>

<p>The original image is convolved with a Gaussian kernel. As described above, the resulting image is a low pass filtered version of the original image. The cut-off frequency can be controlled using the parameter σ, which is standard deviation.</p>

<figure><figcaption>Figure 4: Laplacian Pyramid Implementation. Star represents convolution operation with Gaussian filter and downward arrow represents downsampling of the image.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*DvmPVVn8qAFibpwcmj_ybA.png" /></figure>

<p>The Laplacian is then computed as the difference between the original image and the low pass filtered image i.e. it’s the difference between successive Gaussian pyramid levels. This process is continued to obtain a set of band-pass filtered images (since each is the difference between two levels of the Gaussian pyramid). Thus the Laplacian pyramid is a set of bandpass filters.</p>

<figure><figcaption>Figure 5: I,f represent Gaussian images and h represent Laplacian images</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*MFjKM3pJIBluxOvjDiyZJQ.png" /></figure>

<p>The original image is repeatedly filtered and subsampled to generate the sequence of reduced resolution images. These comprise a set of low pass filtered copies of the original image in which the bandwidth decreases in one-octave steps.</p>

<p>So after using Laplacian pyramid we then perform temporal processing on each spatial band. The time series corresponding to the value of a pixel in a frequency band is passed through a bandpass filter to extract the frequency bands of interest. For heart rate detection, I selected frequencies within 0.4–4Hz, corresponding to 24–240 beats per minute (this was specified in EVM paper itself) to magnify a pulse. The temporal processing is uniform for all spatial levels, and for all pixels within each level that is the time series of every pixel is passed through the same filter.</p>

<h2 id="how-is-it-done">How is it Done?</h2>

<p>Spatial Filtering: It is done to spatially filter the video sequence to obtain basebands of different spatial frequencies. The purpose of spatial filtering is simply to “spelt” multiple adjacent pixels into one piece, a low pass filter can be used. In fact, linear EVM uses Laplacian pyramids or Gaussian pyramids for multiresolution decomposition.</p>

<p>Time Domain Filtering: After obtaining the basebands of different spatial frequencies, bandpass filtering in the time domain is performed for each baseband in order to extract the part of the change signal we are interested in. For example, if we want to amplify the heart rate signal, we can choose bandpass filtering from 0.4 to 4 Hz (24 to 240 bpm). This band is the range of human heart rate.</p>

<h2 id="amplification">Amplification:</h2>

\[\hat I(x,t) = f(x+(1+ \alpha)\delta(t)\]

<p>This represents Color Intensity of the pixels at location x in time t. δ(t) represents the displacement function. α is the amplification factor.</p>

<p>\(f(x + δ(t))\) in a first-order Taylor expansion about x, can be represented as:</p>

\[I(x,t) \approx f(x) + \delta(t) \frac{\partial f(x)}{\partial x}\]

\[f(x) = f(a) + f'(a)(x-a)\]

<p>This is first order Taylor expansion. Here \((x-a)\) is displacement function.</p>

<p>The temporal bandpass filter is selected to pull out the motions or signals that we wish to be amplified.</p>

\[B(x,t) = \delta(t) \frac{\partial f(x)}{\partial x}\]

<p>This is a temporal Bandpass filter which is a result of applying a broadband temporal bandpass filter to \(I(x, t)\) at every position x. For now, \(\delta(t)\) is within the passband of the temporal bandpass filter.</p>

<blockquote>
  <p>Tip: For color amplification of blood flow, a narrow passband produces a more noise-free result.</p>
</blockquote>

<figure><figcaption>Figure 6: The ideal filters (a) and (b) are implemented using DCT (Discrete Cosine Transform). The second-order IIR filter (d) has a broader passband than an ideal filter.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*ET0w7uEXFCneh4BMRVdkgQ.png" /></figure>

<blockquote>
  <p>Important: The Butterworth filter is used to convert a user-specified frequency band into a second-order IIR (infinite impulse response) and is used in our real-time application.</p>
</blockquote>

<p>This shows the frequency response of some of the temporal filters used in the paper. Ideal bandpass filters are used for color amplification as they have passbands with sharp cutoff frequencies.</p>

<p>For pulse detection, after computing Laplacian pyramid the magnification value or amplification factor α, for the finest two levels are set to 0. This causes downsampling and applies a spatial low pass filter to each frame to reduce both quantization and noise and to boost the subtle pulse signal that we are interested in. The incoming video frame is then passed through an ideal bandpass filter with a passband of 0.83 Hz to 1 Hz (50 bpm to 60 bpm). Finally, a large value of α ≈ 100 (amplification factor) and λc ≈ 1000 (<strong>cutoff frequency, beyond which an attenuated version of α is used that either forces α to zero for all λ &lt; λc, or linearly scales α down to zero. This is an important parameter in controlling noise</strong>) was applied to the resulting spatially lowpass signal to emphasize the color change as much as possible. The final video was formed by adding this signal back to the original.</p>

<p><label for="side-note-video" class="margin-toggle"> ⊕</label><input type="checkbox" id="side-note-video" class="margin-toggle" /><span class="marginnote">MIT Computer Program Reveals Invisible Motion in Video | The New York Times </span></p>
<style>
  .embed-container {
    position: relative;
    padding-bottom:56.25%;
    padding-top:30px;
    overflow: hidden;
    max-width: 70%;
    margin-bottom: -23%;
  }
  .embed-container iframe,
  .embed-container object,
  .embed-container embed {
    position: absolute;
    top: 0;
    left: 0;
    width: 80%;
    height: 55%;
  }
</style>

<div class="embed-container">
  <iframe title="YouTube video player" width="480" height="360" src="https://www.youtube.com/embed/3rWycBEHn3s" frameborder="0" allowfullscreen=""></iframe>
</div>

<p>In this video, we can see periodic green to red variations at the heartbeat and how blood perfuses the face.</p>

<p><strong>“Higher α can exaggerate specific motions or color changes at the cost of increased noise.”</strong> In some cases, one can account for color clipping artifacts by attenuating the chrominance components of each frame. This approach achieves this by doing all the processing in the YIQ space. Users can attenuate the chrominance components, I and Q, before conversion to the original color space.</p>

<p><strong><em>“The paper <a href="http://people.csail.mit.edu/mrub/papers/vidmag.pdf">Eulerian Video Magnification for Revealing Subtle Changes in the World</a> is the work of the MIT CSAIL team.”</em></strong></p>

<h2 id="conclusion">Conclusion</h2>

<p>This algorithm can find its use in many aspects of our life like in pulse oximetry which is limited to certain application areas (usually the fingertip) and bears the risk of the probe failing due to the movement of the patient or low perfusion of the hands during long-time recordings. These limitations can be overcome by the analysis of video signals using this technique, which do not depend on contact-based measurement hardware and can be applied to well-circulated body areas (e.g. the head).</p>

<p>The world’s expanding and aging population has created a demand for inexpensive, unobtrusive, automated healthcare solutions. Eulerian Video Magnification (EVM) aids in the development of these solutions by allowing for the extraction of physiological signals from video data. This paper examines the potential of thermal video in conjunction with EVM to extract physiological measures, particularly heart rate.</p>

<blockquote>
  <p><strong>Follow me on Twitter <a href="https://twitter.com/theujjwal9">@theujjwal9</a></strong></p>
</blockquote>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/18/markov-decision-process"><h3 class="contrast">Markov Decision Process: A Framework for Decision Making</h3></a>
          <br /><span class="smaller">December 22, 2018</span>  <br />
          <div><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 110}
});
</script>

<p>Markov Decision Process (MDP) provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. Let’s dive into how MDPs help us understand different types of environments and decision-making scenarios.</p>

<h2 id="types-of-decision-environments">Types of Decision Environments</h2>

<p>Decision environments can be categorized along several dimensions:</p>

<ol>
  <li>Complete vs Incomplete Information</li>
  <li>Fully Observable vs Partially Observable</li>
  <li>Competitive vs Collaborative</li>
  <li>Static vs Dynamic</li>
  <li>Deterministic vs Stochastic</li>
</ol>

<p>Today, we’ll focus on observability in environments and how it impacts decision-making.</p>

<h2 id="fully-observable-environments">Fully Observable Environments</h2>

<p>In a fully observable environment, the agent has complete information about the current state of the system. Mathematically, we can represent this as:</p>

\[s_t = f(o_t)\]

<p>where:</p>
<ul>
  <li>\(s_t\) is the true state at time t</li>
  <li>\(o_t\) is the observation at time t</li>
  <li>\(f\) is a direct mapping function</li>
</ul>

<h3 id="example-chess">Example: Chess</h3>
<p>Consider a chess game. The state space can be represented as:</p>

\[S = \{s_1, s_2, ..., s_n\}\]

<p>where each \(s_i\) represents a possible board configuration. The optimal policy \(\pi^*\) depends only on the current state:</p>

\[\pi^*(s) = \arg\max_a Q(s,a)\]

<p>where \(Q(s,a)\) is the action-value function representing the expected future reward.</p>

<h2 id="partially-observable-environments">Partially Observable Environments</h2>

<p>In partially observable environments, the agent cannot directly observe the true state. Instead, it maintains a belief state based on past observations. This can be represented as:</p>

\[b_t = P(s_t | o_1, o_2, ..., o_t)\]

<p>where:</p>
<ul>
  <li>\(b_t\) is the belief state at time t</li>
  <li>\(s_t\) is the true state</li>
  <li>\(o_1, o_2, ..., o_t\) are observations up to time t</li>
</ul>

<h3 id="example-poker">Example: Poker</h3>
<p>In poker, the optimal policy depends on both current observation and history:</p>

\[\pi^*(b_t) = \arg\max_a \sum_{s \in S} b_t(s)Q(s,a)\]

<p>where \(b_t(s)\) represents the probability of being in state \(s\) given the history of observations.</p>

<h2 id="the-markov-property">The Markov Property</h2>

<p>The key distinction lies in the Markov property. In fully observable environments:</p>

\[P(s_{t+1}|s_t, s_{t-1}, ..., s_1) = P(s_{t+1}|s_t)\]

<p>This means the future is independent of the past given the present. However, in partially observable environments, we need to maintain a history:</p>

\[P(s_{t+1}|o_t, o_{t-1}, ..., o_1) \neq P(s_{t+1}|o_t)\]

<h2 id="real-world-applications">Real-World Applications</h2>

<p>Consider a self-driving car:</p>
<ul>
  <li><strong>Fully Observable</strong>: Highway driving with clear weather and perfect sensor data</li>
  <li><strong>Partially Observable</strong>: Urban driving in fog where past observations help predict hidden obstacles</li>
</ul>

<p>The mathematical framework for decision-making changes based on observability:</p>

\[V(s) = \max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a)V(s')\right]\]

<p>where:</p>
<ul>
  <li>\(V(s)\) is the value function</li>
  <li>\(R(s,a)\) is the reward function</li>
  <li>\(\gamma\) is the discount factor</li>
  <li>\(P(s'|s,a)\) is the transition probability</li>
</ul>

<p>This equation, known as the Bellman equation, takes different forms depending on whether we’re dealing with fully or partially observable environments.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Understanding the observability of an environment is crucial for designing effective decision-making systems. While fully observable environments like chess can rely solely on current state information, partially observable environments like poker require sophisticated belief state tracking and historical information processing.</p>

<p>The mathematical frameworks we’ve explored help us design better algorithms for each type of environment, leading to more effective decision-making systems in real-world applications.</p>
</div> 
        </li>
          
        <li class="listing">
          
            <hr class="slender" />
          
          <a href="/articles/18/knowledge-distillation"><h3 class="contrast">Knowledge Distillation</h3></a>
          <br /><span class="smaller">May 4, 2018</span>  <br />
          <div><p>This blog first appeared at Intel Devpost. <a href="https://software.intel.com/content/www/us/en/develop/articles/knowledge-distillation-with-keras.html">Here is the link</a></p>

<p>Hinton, Geoffrey, et al. “Distilling the Knowledge in a Neural Network.” arXiv, 9 Mar. 2015, arxiv.org/abs/1503.02531v1.</p>

<p><a href="https://arxiv.org/abs/1503.02531v1">Link to paper</a></p>

<p>Associated code can be found on <a href="https://github.com/Ujjwal-9/Knowledge-Distillation">Github</a>.</p>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  <!-- tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, -->
  jax: ["input/TeX","output/HTML-CSS"],
  displayAlign: "left",
  "HTML-CSS": { scale: 110}
});
</script>

<h1 id="abstract">Abstract</h1>

<p>The problem we are facing right now is that we have built sophisticated models that can perform complex tasks, but the question is: how do we deploy such bulky models on our mobile devices for instant usage? Obviously, we can deploy our model to the cloud and call it whenever we need its service, but this would require a reliable internet connection and hence becomes a constraint in production. So what we need is a model that can run on our mobile devices.</p>

<figure><figcaption>Deep learning models in production</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*vCILduBp-gylqOp7WUme0Q.png" /></figure>

<p>So what’s the problem? We can train a small network that can run on the limited computational resources of our mobile device. But there is a problem with this approach. Small models can’t extract many complex features that can be handy in generating predictions unless you devise some elegant algorithm to do so. Though ensembles of small models give good results, unfortunately making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users. In this case, we resort to either of these 2 techniques:</p>

<ul>
  <li>Knowledge Distillation</li>
  <li>Model Compression</li>
</ul>

<p>If you have developed a better solution or if I might have missed something, please mention it in the comments 🙂</p>

<p>In this blog, we will look at <strong>Knowledge Distillation</strong>. I will cover model compression in an upcoming blog.</p>

<p>Knowledge distillation is a simple way to improve the performance of deep learning models on mobile devices. In this process, we train a large and complex network or an ensemble model which can extract important features from the given data and can, therefore, produce better predictions. Then we train a small network with the help of the cumbersome model. This small network will be able to produce comparable results, and in some cases, it can even be made capable of replicating the results of the cumbersome network.</p>

<figure class="fullwidth"><img src="https://cdn-images-1.medium.com/max/2456/1*r_eguFXxHkAzDRu8tM-95g.jpeg" /><figcaption>GoogleNet</figcaption></figure>

<p>For example, since GoogLeNet is a very cumbersome (meaning deep and complex) network, its depth gives it the ability to extract complex features and its complexity gives it the power to remain accurate. But the model is heavy enough that one would surely need a large amount of memory and a powerful GPU to perform large and complex calculations. That’s why we need to transfer the knowledge learned by this model to a much smaller model which can easily be used on mobile devices.</p>

<h1 id="about-cumbersome-models">About Cumbersome Models</h1>

<p>Cumbersome models learn to discriminate between a large number of classes. The normal <strong>training objective</strong> is to maximize the average log probability of the correct answer, and it assigns probabilities to all the classes, with some classes given small probabilities with respect to others. The relative probabilities of incorrect answers tell us a lot about how this complex model tends to generalize. An image of a car, for example, may only have a very small chance of being mistaken for a truck, but that mistake is still many times more probable than mistaking it for a cat.</p>
<blockquote>
  <p>Note that the objective function should be chosen such that it generalizes well to new data. So it should be kept in mind while selecting an appropriate objective function that it shouldn’t be selected in such a way that it only optimizes well on training data.</p>
</blockquote>

<p>Since these operations will be quite heavy for mobile devices during performance, to deal with this situation, we have to transfer the knowledge of the cumbersome model to a small model which can be easily exported to mobile devices. To achieve this, we can consider the cumbersome model as the <strong>Teacher Network</strong> and our new small model as the <strong>Student Network.</strong></p>

<h1 id="teacher-and-student">Teacher and Student</h1>

<p>You can ‘distill’ the large and complex network into another much smaller network, and the smaller network does a reasonable job of approximating the original function learned by the deep network.</p>

<figure><figcaption>Teacher Student Architecture</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*6G6HHityX_zBgrFfR_z-UQ.png" /></figure>

<p>However, there is a catch: the distilled model (<strong>student</strong>) is trained to mimic the output of the larger network (<strong>teacher</strong>), instead of training it on the raw data directly. This has something to do with how the deeper network learns hierarchical abstractions of the features.</p>

<h1 id="so-how-is-this-transfer-of-knowledge-done">So how is this transfer of knowledge done?</h1>

<figure><figcaption>Knowledge Transfer between Student and Teacher.</figcaption><img src="https://cdn-images-1.medium.com/max/2964/1*WxFiH3XDY1-28tbyi4BGDA.png" /></figure>

<p>The transferring of the generalization ability from the cumbersome model to a small model can be done by using the class probabilities produced by the cumbersome model as “soft targets” for training the small model. For this transfer stage, we use the same training set or a separate “transfer” set as used for training the cumbersome model. When the cumbersome model is a large ensemble of simpler models, we can use the arithmetic or geometric mean of their individual predictive distributions as the soft targets. When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much less data than the original cumbersome model while using a much higher learning rate.</p>

<figure><figcaption>How soft logits give an idea of resemblance between classes.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*ekrPR2eYbD2Y9HWTV5YGxw.jpeg" /></figure>

<p>Much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. This is valuable information that defines a rich similarity structure over the data (i.e., it says which 2’s look like 3’s and which look like 7’s, or which “golden retriever” looks like a “Labrador”) but it has very little influence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero.</p>

<h1 id="distillation">Distillation</h1>

<p>For distilling the learned knowledge, we use <strong>Logits</strong> (the inputs to the final softmax). Logits can be used for learning the small model, and this can be done by minimizing the squared difference between the logits produced by the cumbersome model and the logits produced by the small model. The equation below represents softmax with temperature.</p>

\[P_t(a) = \frac{\exp(q_t(a)/\tau)}{\sum_{i=1}^n\exp(q_t(i)/\tau)}\]

<p>For high temperatures (\(\tau \rightarrow \infty\)), all actions have nearly the same probability, and at lower temperatures (\(\tau \rightarrow 0\)), the expected rewards affect the probability more. For low temperature, the probability of the action with the highest expected reward tends to 1.</p>

<p>In distillation, we raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets. We then use the same high temperature when training the small model to match these soft targets.</p>

<h1 id="objective-function">Objective Function</h1>

<p>The first objective function is the cross-entropy with the soft targets, and this cross-entropy is computed using the same high temperature in the softmax of the distilled model as was used for generating the soft targets from the cumbersome model.</p>

<p>The second objective function is the cross-entropy with the correct labels, and this is computed using exactly the same logits in softmax of the distilled model but at a temperature of 1.</p>

<figure><figcaption>Objective of deep learning models is to generalize.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*rbi3dpUQaQjI-ezbyDzhug.png" /></figure>

<h1 id="training-ensembles-of-specialists">Training ensembles of specialists</h1>

<p>Training an ensemble of models is a very simple way to take advantage of parallel computation. But there is an objection that an ensemble requires too much computation at test time. However, this can be easily dealt with using the technique we are learning. And so “Distillation” can be used to deal with this issue.</p>

<figure><figcaption>Using large models to train small and simpler models.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*aIBLpCWRF5J1kbXE_s9KcQ.png" /></figure>

<h1 id="specialist-models"><strong>Specialist Models</strong></h1>

<p><em>Specialist models and one generalist model make up our one cumbersome model</em>. The Generalist Model is trained on all training data, and Specialist Models focus on different confusable subsets of the classes to reduce the total amount of computation required to learn an ensemble. The main problem with specialists is that they overfit very easily. But this overfitting may be prevented by using soft targets.</p>

<h1 id="reduce-overfitting-in-specialist-models">Reduce Overfitting in Specialist Models</h1>

<p>To reduce overfitting and share the work of learning lower-level feature detectors, each specialist model is initialized with the weights of the generalist model. These weights are then slightly modified by training the specialist, with half its examples coming from its special subset, and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled.</p>

<figure><figcaption>How we reduce overfitting. One example.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*TDMCC6ZHzxQo-pn6Y-ZZWA.png" /></figure>

<h1 id="assign-classes-to-specialists">Assign classes to Specialists</h1>

<p>We apply a clustering algorithm to the covariance matrix of the predictions of our generalist model so that a set of classes Sm that are often predicted together will be used as targets for one of our specialist models, m. So we apply K-means clustering to the columns of the covariance matrix to get our required clusters or classes.</p>

<figure><figcaption>Assign a score to an ordered covariance matrix. High correlations within a cluster improve the score. High correlations between clusters decrease the score.</figcaption><img src="https://cdn-images-1.medium.com/max/2000/1*Coch85xMgRVk6UbS5zjzVg.png" /></figure>

<blockquote>
  <p>Covariance/Correlation clustering provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance.</p>
</blockquote>

<h1 id="performing-inference">Performing inference</h1>

<p>For each test case, we find the ‘n’ most probable classes according to the generalist model. Call this set of classes k.</p>

<p>We then take all the specialist models, m, whose special subset of confusable classes, Sm, has a non-empty intersection with k and call this the active set of specialists Ak (note that this set may be empty). We then find the full probability distribution q over all the classes that minimizes:</p>

\[KL(p^g, q) + \sum_{m \epsilon A_k} KL(p^m, q)\]

<p>KL denotes the KL-divergence. \(p^m\), \(p^g\) denote the probability distribution of a specialist model or the generalist full model.</p>

\[KL(p||q) = \sum_{i}p_i \log {p_i \over q_i}\]

<p>The distribution \(p^m\) is over all the specialist classes of \(m\) plus a single dustbin class, so when computing its \(KL\) divergence from the full \(q\) distribution, we sum all of the probabilities that the full \(q\) distribution assigns to all the classes in \(m\)’s dustbin.</p>

<h1 id="soft-targets-as-regularizers">Soft Targets as Regularizers</h1>

<p>Soft Targets or labels predicted from a model contain more information than binary hard labels due to the fact that they encode similarity measures between the classes.</p>

<p>Incorrect labels tagged by the model describe co-label similarities, and these similarities should be evident in future stages of learning, even if the effect is diminished. For example, imagine training a deep neural net on a classification dataset of various dog breeds. In the initial few stages of learning, the model will not accurately distinguish between similar dog breeds such as a Belgian Shepherd versus a German Shepherd. This same effect, although not so exaggerated, should appear in later stages of training. If given an image of a German Shepherd, the model predicts the class German Shepherd with high accuracy, the next highest predicted dog should still be a Belgian Shepherd or a similar-looking dog. Over-fitting starts to occur when the majority of these co-label effects begin to disappear. By forcing the model to maintain these effects in the later stages of training, we reduce the amount of over-fitting. Though using soft targets as Regularizers is not considered very effective. Check out <a href="https://neptune.ai/blog/knowledge-distillation">this blog</a> by neptune.ai to get more ideas about different algorithms used for knowledge distillation.</p>

<blockquote>
  <p><strong>Follow me on twitter <a href="https://twitter.com/theujjwal9">@theujjwal9</a></strong></p>
</blockquote>
</div> 
        </li>
    
  </ul>

    </article>
    <span class="print-footer">blog - Ujjwal Upadhyay</span>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<!-- <style>

i:hover{
  -webkit-animation: glow 2s ease-in-out infinite alternate;
  -moz-animation: glow 2s ease-in-out infinite alternate;
  animation: glow 2s ease-in-out infinite alternate;
}

@-webkit-keyframes glow {
  from {
    text-shadow: 0 0 10px #fff, 0 0 20px #fff, 0 0 30px #e60073, 0 0 40px #e60073, 0 0 50px #e60073, 0 0 60px #e60073, 0 0 70px #e60073;
  }
  
  to {
    text-shadow: 0 0 20px #fff, 0 0 30px #ff4da6, 0 0 40px #ff4da6, 0 0 50px #ff4da6, 0 0 60px #ff4da6, 0 0 70px #ff4da6, 0 0 80px #ff4da6;
  }

</style> -->

<footer>
  <hr class="slender">
  <ul class="footer-links" style="font-size: 1.6rem;">
    <li><a href="mailto:ujjwalupadhyay8@gmail.com"><i class="fa fa-envelope"></i></a></li>
    <li><a href="https://twitter.com/theujjwal9"><i class="fa fa-twitter"></i></a></li>
    <li><a href="https://github.com/ujjwal-9"><i class="fa fa-github"></i></a></li>
    <li><a href="https://www.linkedin.com/in/ujjwal-9/"><i class="fa fa-linkedin"></i></a></li>
    <li><a href="https://scholar.google.com/citations?user=lvpaXdEAAAAJ&hl=en"><i class="fa fa-graduation-cap"></i></a></li>
    <li><a href="https://ujjwal-9.github.io/feed.xml"><i class="fa fa-rss"></i></a></li>
    <!-- 
      <li>
        <a href="https://github.com/ujjwal-9"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="https://linkedin.com/in/ujjwal-9"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="/feed.xml"><span class="icon-rss2"></span></a>
      </li>
       -->
  </ul>
<div class="credits">
<span>&copy; 2024 &nbsp;&nbsp;UJJWAL UPADHYAY</span></br> <br>
</div>  
</footer>
  </body>
</html>
