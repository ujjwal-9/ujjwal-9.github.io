<!DOCTYPE html>
<html>
  <head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-70SVKC0TJP"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-70SVKC0TJP');
  </script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Ujjwal Upadhyay - Research</title>
  <meta name="description" content="Personal webpage">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <link rel="stylesheet" type="text/css" href="/css/icomoon.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="/research">

  <link rel="alternate" type="application/rss+xml" title="Ujjwal Upadhyay" href="/feed.xml" />
  <link rel="icon" type="image/svg" href="/assets/img/infinite.svg">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Research" />
<meta name="author" content="Ujjwal Upadhyay" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal webpage" />
<meta property="og:description" content="Personal webpage" />
<meta property="og:site_name" content="Ujjwal Upadhyay" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Research" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Ujjwal Upadhyay"},"description":"Personal webpage","headline":"Research","url":"/research"}</script>
<!-- End Jekyll SEO tag -->


  

  

  

  
</head>

  <body class="full-width">
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <ul class="topnav" style="float:left">
            <li><a style="padding-top:.5em" href="/"><strong>Ujjwal Upadhyay</strong></a></li>
        </ul>
	<ul class="topnav">    
        
	
            
            
            
  	
            
            
	        
                <li><a class="active right" href="/research">Research</a></li>
	        
	    
            
  	
            
            
	        
                <li><a class="right" href="/writings">Writings</a></li>
	        
	    
            
  	
            
            
	        
                <li><a class="right" href="/scratchpad">Scratchpad</a></li>
	        
	    
            
  	
            
            
            
  	
            
  	
            
  	
            
  	
        <!-- <li><a class="right" href="https://drive.google.com/file/d/13Hxj3cRTK9nMl54z58sPOoOah6iEtw-C/view?usp=sharing">cv</a></li> -->
	</ul>
	</nav>
        <hr class="slender">
</header>

    <article>
    
      <p><a href="https://scholar.google.com/citations?user=lvpaXdEAAAAJ&amp;hl=en">Google Scholar Profile</a></p>

<h2 id="3dcompat">3DCoMPaT</h2>
<p class="paper">
    <em><a href="https://3dcompat-dataset.org/">3DCoMPaT: Composition of Materials on Parts of 3D Things</a></em><br />
    Published at ECCV 2022 Oral Track. <br />
    Yuchen Li<sup>*</sup>, <strong>Ujjwal Upadhyay</strong><sup>*</sup>, Habib Slim<sup>*</sup>, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka, Mohamed Elhoseiny
</p>

<p><code class="language-plaintext highlighter-rouge">We present 3D CoMPaT, a richly annotated large-scale dataset of more than 7.19 million rendered compositions of Materials on Parts of 7262 unique 3D Models; 990 compositions per model on average. 3DCoMPaT covers 43 shape categories, 235 unique part names, and 167 unique material classes that can be applied to parts of 3D objects. Each object with the applied part-material compositions is rendered from four equally spaced views as well as four randomized views, leading to a total of 58 million renderings (7.19 million compositions ×8 views). This dataset primarily focuses on stylizing 3D shapes at part-level with compatible materials. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects.</code></p>

<h2 id="3dreftransformer">3DRefTransformer</h2>
<p class="paper">
    <em><a href="https://openaccess.thecvf.com/content/WACV2022/papers/Abdelreheem_3DRefTransformer_Fine-Grained_Object_Identification_in_Real-World_Scenes_Using_Natural_Language_WACV_2022_paper.pdf">3DRefTransformer: Fine-Grained Object Identification in Real-World Scenes Using Natural Language</a></em><br />
    Published at WACV 2022. <br />
    Ahmed Abdelreheem, <strong>Ujjwal Upadhyay</strong>, Ivan Skorokhodov, Rawan Al Yahya, Jun Chen, Mohamed Elhoseiny
</p>

<p><code class="language-plaintext highlighter-rouge">In this paper, we study fine-grained 3D object identification in real-world scenes described by a textual query. The task aims to discriminatively understand an instance of a particular 3D object described by natural language utterances among other instances of 3D objects of the same class appearing in a visual scene. We introduce the 3DRefTransformer net, a transformer-based neural network that identifies 3D objects described by linguistic utterances in real-world scenes. The network's input is 3D object segmented point cloud images representing a real-world scene and a language utterance that refers to one of the scene objects. The goal is to identify the referred object.</code></p>

<h2 id="deep-aspects">Deep-ASPECTS</h2>
<p class="paper">
    <em><a href="https://arxiv.org/abs/2203.03622">Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement</a></em><br />
    Published at ECCV 2022. <br />
    <strong>Ujjwal Upadhyay</strong>, Mukul Ranjan, Satish Golla, Swetha Tanamala, Preetham Sreenivas, Sasank Chilamkurthy, Jeyaraj Pandian, Jason Tarpley
</p>

<p><code class="language-plaintext highlighter-rouge">The quick onset of a focused neurological deficit caused by interruption of blood flow in the territory supplied by the MCA is known as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is used to estimate the extent of early ischemic changes in patients with MCA stroke. This study proposes a deep learning-based method to score the CT scan for ASPECTS. Our work has three highlights. First, we propose a novel method for medical image segmentation for stroke detection. Second, we show the effectiveness of AI solution for fully-automated ASPECT scoring with reduced diagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a dice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72 for the infarcts segmentation. Lastly, we show that our model's performance is inline with inter-reader variability between radiologists.</code></p>

<h2 id="ai-based-gaze-deviation-detection-to-aid-lvo-diagnosis-in-ncct">AI based gaze deviation detection to aid LVO diagnosis in NCCT</h2>

<p class="paper">
    <em><a href="https://jnis.bmj.com/content/14/Suppl_1/A41">AI based gaze deviation detection to aid LVO diagnosis in NCCT</a></em><br />
    Published at Society of NeuroInterventional Surgery (SNIS) 2022. <br />
    <strong>Ujjwal Upadhyay</strong>, Satish Golla, Shubham Kumar, Kamila Szweda, Reza Shahripour, Jason Tarpley
</p>

<p><code class="language-plaintext highlighter-rouge">Stroke caused by emergent large vessel occlusion (LVO) is a critical time-sensitive diagnosis requiring prompt identification to identify candidates for endovascular therapy (EVT). As a result, identifying imaging findings on non-contrast computed tomography (NCCT) that are predictive of LVO would aid in the identification of potential EVT candidates. We present and validate gaze deviation as an indicator for detecting LVO using NCCT. In addition, we present an Artificial Intelligence (AI) algorithm for detecting this indicator.</code></p>

<h2 id="lvo-detection-on-ct-angiography-using-deep-learning">LVO Detection on CT Angiography using Deep Learning</h2>

<p class="paper">
    <em><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=lvpaXdEAAAAJ&amp;citation_for_view=lvpaXdEAAAAJ:_FxGoFyzp5QC">Deep Learning Based LVO Detection on CT Angiography of Brain</a></em><br />
    Published at International Journal of Stroke 2022. <br />
    Shubham Kumar, Arjun Agarwal, Swetha Tanamala, Satish Golla, Preetham Putha, <strong>Ujjwal Upadhyay</strong>, Sasank Chilamkurthy, Jeyaraj Pandian
</p>

<h2 id="arterial-input-function-estimation-on-ct-perfusion">Arterial Input Function Estimation on CT Perfusion</h2>

<p class="paper">
    <em><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=lvpaXdEAAAAJ&amp;citation_for_view=lvpaXdEAAAAJ:Y0pCki6q_DkC">Machine Learning Approach to Arterial Input Function Estimation in Cerebral Perfusion Imaging</a></em><br />
    Published at ECCV 2022. <br />
    Ravi Kushawaha, Sasank Chilamkurthy, Satish Golla, <strong>Ujjwal Upadhyay</strong>, Swetha Tanamala, Preetham Putha, Ken Butcher
</p>

<h2 id="latent-space-poisoning">Latent Space Poisoning</h2>
<p>It’s an adversarial attack which used JointVAE architecture to search the latent space for potential examples which can fool the black box classifer. The work ended up beating PGD based defenses on various datasets (MNIST, CelebA, and SVHN) and reducing its success rate by ~70%.</p>

<p class="paper">
    <em><a href="https://arxiv.org/abs/2012.05027">Generating Out of Distribution Adversarial Attack using Latent Space Poisoning</a></em><br />
    Published at IEEE SPL 2021. <br />
    <strong>Ujjwal Upadhyay</strong>, Prerana Mukherjee
</p>

<p><code class="language-plaintext highlighter-rouge">In this paper, we propose a novel mechanism of generating adversarial examples where the actual image is not corrupted rather its latent space representation is utilized to tamper with the inherent structure of the image while maintaining the perceptual quality intact and to act as legitimate data samples. As opposed to gradient-based attacks, the latent space poisoning exploits the inclination of classifiers to model the independent and identical distribution of the training dataset and tricks it by producing out of distribution samples. We train a disentangled variational autoencoder (beta-VAE) to model the data in latent space and then we add noise perturbations using a class-conditioned distribution function to the latent space under the constraint that it is misclassified to the target label.</code></p>

<h2 id="stroke-care-with-ai-in-lmics">Stroke Care with AI in LMIC’s</h2>
<p class="paper">
    <em><a href="https://www.jns-journal.com/article/S0022-510X(21)02356-X/fulltext">AI improves stroke diagnosis and care at a low resource hospital in India</a></em><br />
    Published at Journal of the Neurological Sciences 2021. <br />
    <strong>Ujjwal Upadhyay</strong>, Jemin Webster, Swetha Tanamala, Sasank Chilamkurthy, Satish Golla, Justy Antony Chiramal
</p>

<p><code class="language-plaintext highlighter-rouge">AI solution (qER) was developed to report abnormalities given a non-contrast CT (NCCT). qER compiles findings into a detailed report highlighting abnormalities, indicating affected brain region along with estimated volume for bleeds. These findings coupled with clinical symptoms helps identify stroke.</code></p>

<h2 id="emospeech-command-dataset">EmoSpeech Command Dataset</h2>
<p>In collaboration with the IIT Delhi, Marconi Society and Google, My team developed and performed validation of a predictive model to target safety systems on edge devices. The source code is available on GitHub. Our work was reported in the <a href="https://www.indiatoday.in/education-today/news/story/delhi-college-students-win-prestigious-marconi-awards-for-apps-on-women-safety-and-checking-air-pollution-1611854-2019-10-22">India Today</a>, <a href="https://www.thehindubusinessline.com/news/variety/delhi-students-solutions-for-womens-safety-air-pollution-bag-marconi-societys-celestini-programme-awards/article29765200.ece">The Hindu</a>, and <a href="https://www.financialexpress.com/industry/technology/benefits-of-connectivity-students-win-marconi-awards-for-innovative-apps/1747344/">Financial Express</a> among many others.</p>
<p class="paper">
    <em><a href="https://arxiv.org/abs/1910.13801">Indian EmoSpeech Command Dataset: A dataset for emotion based speech recognition in the wild</a></em><br />
    Submitted at Computer Speech and Language Journal.<br />
    Subham Banga<sup>*</sup>, <strong>Ujjwal Upadhyay</strong><sup>*</sup>, Piyush Agarwal<sup>*</sup>, Aniket Sharma<sup>*</sup>, Prerana Mukherjee
</p>

<p><code class="language-plaintext highlighter-rouge">Speech emotion analysis is an important task which further enables several application use cases. The non-verbal sounds within speech utterances also play a pivotal role in emotion analysis in speech. Due to the widespread use of smartphones, it becomes viable to analyze speech commands captured using microphones for emotion understanding by utilizing on-device machine learning models. The non-verbal information includes the environment background sounds describing the type of surroundings, current situation and activities being performed. In this work, we consider both verbal (speech commands) and non-verbal sounds (background noises) within an utterance for emotion analysis in real-life scenarios. We create an indigenous dataset for this task namely "Indian EmoSpeech Command Dataset". It contains keywords with diverse emotions and background sounds, presented to explore new challenges in audio analysis.</code></p>

<h2 id="video-aggression-net">Video Aggression Net</h2>
<p>The project aims at identifying agressive activity from video.</p>
<p class="paper">
    <em><a href="">Video-AggNet: Fine-grained aggressive activity recognition in the wild</a></em><br />
    Submitted at IEEE ICME 2021.<br />
    <strong>Ujjwal Upadhyay</strong>, Prerana Mukherjee, Anupama Ray, Ritu Garg
</p>

<p><code class="language-plaintext highlighter-rouge">In this paper, we propose an end-to-end attention guided fine-grained aggressive activity recognition framework namely \textit{Video-AggNet}. We make an inherent assumption that criminal activities that are recorded in CCTV cameras are highly influenced by the location of the objects and their movement that might affect other objects in future timestamp. In order to handle this, we provide a keyframe selection strategy which is used to sample out the relevant frames and reduce processing.</code></p>

<!-- ## Algorithmic Fairness
<p class="paper" markdown="1">
*[Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions](https://arxiv.org/abs/1811.07867)*<br/>
To Appear in Annual Review of Statistics, 2021<br/>
S Mitchell, **E Potash**, S Barocas, A D'Amour, K Lum
</p> -->

    </article>
    <span class="print-footer">Research - Ujjwal Upadhyay</span>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<!-- <style>

i:hover{
  -webkit-animation: glow 2s ease-in-out infinite alternate;
  -moz-animation: glow 2s ease-in-out infinite alternate;
  animation: glow 2s ease-in-out infinite alternate;
}

@-webkit-keyframes glow {
  from {
    text-shadow: 0 0 10px #fff, 0 0 20px #fff, 0 0 30px #e60073, 0 0 40px #e60073, 0 0 50px #e60073, 0 0 60px #e60073, 0 0 70px #e60073;
  }
  
  to {
    text-shadow: 0 0 20px #fff, 0 0 30px #ff4da6, 0 0 40px #ff4da6, 0 0 50px #ff4da6, 0 0 60px #ff4da6, 0 0 70px #ff4da6, 0 0 80px #ff4da6;
  }

</style> -->

<footer>
  <hr class="slender">
  <ul class="footer-links" style="font-size: 1.6rem;">
    <li><a href="mailto:ujjwalupadhyay8@gmail.com"><i class="fa fa-envelope"></i></a></li>
    <li><a href="https://twitter.com/theujjwal9"><i class="fa fa-twitter"></i></a></li>
    <li><a href="https://github.com/ujjwal-9"><i class="fa fa-github"></i></a></li>
    <li><a href="https://www.linkedin.com/in/ujjwal-9/"><i class="fa fa-linkedin"></i></a></li>
    <li><a href="https://scholar.google.com/citations?user=lvpaXdEAAAAJ&hl=en"><i class="fa fa-graduation-cap"></i></a></li>
    <li><a href="https://ujjwal-9.github.io/feed.xml"><i class="fa fa-rss"></i></a></li>
    <!-- 
      <li>
        <a href="https://github.com/ujjwal-9"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="https://linkedin.com/in/ujjwal-9"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="/feed.xml"><span class="icon-rss2"></span></a>
      </li>
       -->
  </ul>
<div class="credits">
<span>&copy; 2024 &nbsp;&nbsp;UJJWAL UPADHYAY</span></br> <br>
</div>  
</footer>
  </body>
</html>
